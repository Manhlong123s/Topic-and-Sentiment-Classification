{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91310,"databundleVersionId":10780789,"sourceType":"competition"},{"sourceId":11050229,"sourceType":"datasetVersion","datasetId":6884090}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T10:01:22.164708Z","iopub.execute_input":"2025-03-25T10:01:22.164979Z","iopub.status.idle":"2025-03-25T10:01:22.521012Z","shell.execute_reply.started":"2025-03-25T10:01:22.164947Z","shell.execute_reply":"2025-03-25T10:01:22.520035Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dpl-302-m-ai-1810-assignment-2/train data.json\n/kaggle/input/dpl-302-m-ai-1810-assignment-2/test.csv\n/kaggle/input/datata/train data.json\n/kaggle/input/datata/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nPh√¢n lo·∫°i t√¨nh c·∫£m vƒÉn b·∫£n ti·∫øng Vi·ªát s·ª≠ d·ª•ng BiLSTM\nT·ªëi ∆∞u cho t√°c v·ª• ph√¢n lo·∫°i t√¨nh c·∫£m (sentiment classification)\n\"\"\"\n\nimport os\nimport re\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom collections import Counter\nimport unicodedata  # Th√™m th∆∞ vi·ªán x·ª≠ l√Ω Unicode\n\n# C·∫•u h√¨nh\nMAX_LENGTH = 100\nVOCAB_SIZE = 20000\nEMBEDDING_DIM = 128\nBATCH_SIZE = 32\nEPOCHS = 15\nLEARNING_RATE = 0.001  # Kh√¥i ph·ª•c learning rate ban ƒë·∫ßu\n\nclass SentimentAttention(tf.keras.layers.Layer):\n    \"\"\"L·ªõp Attention t√πy ch·ªânh cho ph√¢n lo·∫°i c·∫£m x√∫c\"\"\"\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(SentimentAttention, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        # ƒê·∫£m b·∫£o input_shape c√≥ th·ªÉ l√† 2D ho·∫∑c 3D\n        feature_dim = input_shape[-1]\n        \n        # T·∫°o ma tr·∫≠n tr·ªçng s·ªë v√† bias\n        self.W = self.add_weight(\n            name=\"attention_weight\",\n            shape=(feature_dim, 1),\n            initializer=\"glorot_uniform\"\n        )\n        self.b = self.add_weight(\n            name=\"attention_bias\",\n            shape=(1,),\n            initializer=\"zeros\"\n        )\n        self.built = True\n        \n    def call(self, inputs, mask=None):\n        # Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa inputs\n        input_shape = tf.shape(inputs)\n        \n        # N·∫øu ch·ªâ c√≥ 2 chi·ªÅu (batch_size, features) th√¨ √°p d·ª•ng attention tr·ª±c ti·∫øp\n        if len(inputs.shape) == 2:\n            # Khi input l√† t·ª´ dense layer, √°p d·ª•ng dot product m·ªôt l·∫ßn\n            logits = tf.matmul(inputs, self.W) + self.b  # shape=(batch_size, 1)\n            attention_weights = tf.nn.sigmoid(logits)  # S·ª≠ d·ª•ng sigmoid thay v√¨ softmax cho 1 ph·∫ßn t·ª≠\n            \n            # T·∫°o output = input (kh√¥ng c·∫ßn weighted sum v√¨ ch·ªâ c√≥ 1 vector ƒë·∫∑c tr∆∞ng)\n            output = inputs\n            \n            return output, tf.squeeze(attention_weights, axis=-1)\n        \n        # X·ª≠ l√Ω cho tensor 3D (batch_size, timesteps, features)\n        else:\n            # √Åp d·ª•ng tanh(Wx + b) cho m·ªói timestep\n            uit = tf.tanh(tf.matmul(\n                tf.reshape(inputs, [-1, input_shape[-1]]),  # Reshape ƒë·ªÉ dot product\n                self.W\n            ) + self.b)\n            \n            # Reshape v·ªÅ (batch_size, timesteps, 1)\n            uit = tf.reshape(uit, [-1, input_shape[1], 1])\n            \n            # √Åp d·ª•ng mask n·∫øu c√≥\n            if mask is not None:\n                mask = tf.cast(mask, tf.bool)\n                mask = tf.expand_dims(mask, axis=-1)  # (batch_size, timesteps, 1)\n                paddings = tf.ones_like(uit) * (-1e9)\n                uit = tf.where(mask, uit, paddings)\n            \n            # T√≠nh softmax theo chi·ªÅu timesteps\n            attention_weights = tf.nn.softmax(uit, axis=1)  # (batch_size, timesteps, 1)\n            \n            # √Åp d·ª•ng attention weights cho inputs\n            weighted_input = inputs * attention_weights\n            \n            # T·ªïng h·ª£p theo chi·ªÅu timesteps\n            output = tf.reduce_sum(weighted_input, axis=1)  # (batch_size, features)\n            \n            return output, tf.squeeze(attention_weights, axis=-1)\n    \n    def compute_output_shape(self, input_shape):\n        # Output l√† (batch_size, features) v√† (batch_size, sequence_length)\n        if len(input_shape) == 3:\n            return [(input_shape[0], input_shape[2]), (input_shape[0], input_shape[1])]\n        else:  # Tr∆∞·ªùng h·ª£p 2D\n            return [(input_shape[0], input_shape[1]), (input_shape[0], 1)]\n    \n    def get_config(self):\n        config = super(SentimentAttention, self).get_config()\n        return config\n\nclass VietnameseTextClassifier:\n    def __init__(self):\n        print(\"Kh·ªüi t·∫°o Vietnamese Text Classifier v·ªõi m√¥ h√¨nh BiLSTM...\")\n        self.tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n        self.topic_threshold = 0.2  # Gi·∫£m ng∆∞·ª°ng ch·ªß ƒë·ªÅ t·ª´ 0.5 xu·ªëng 0.2\n        \n    def load_train_data(self, json_path):\n        \"\"\"T·∫£i d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ file JSON (h·ªó tr·ª£ ƒë·ªãnh d·∫°ng Label Studio)\"\"\"\n        print(f\"ƒêang t·∫£i d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ {json_path}...\")\n        \n        try:\n            # Ki·ªÉm tra file t·ªìn t·∫°i\n            if not os.path.exists(json_path):\n                print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file {json_path}\")\n                return pd.DataFrame({'text': [], 'sentiment': [], 'topics': []})\n            \n            # T·∫£i d·ªØ li·ªáu JSON\n            with open(json_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            # In th√¥ng tin debug\n            print(f\"C·∫•u tr√∫c d·ªØ li·ªáu: {type(data)}\")\n            if isinstance(data, list):\n                print(f\"S·ªë l∆∞·ª£ng m·ª•c: {len(data)}\")\n            \n            # X·ª≠ l√Ω d·ªØ li·ªáu\n            rows = []\n            \n            # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng Label Studio\n            if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict) and 'annotations' in data[0]:\n                print(\"Ph√°t hi·ªán ƒë·ªãnh d·∫°ng Label Studio\")\n                \n                for item in data:\n                    # Tr√≠ch xu·∫•t vƒÉn b·∫£n\n                    text = \"\"\n                    if 'data' in item and isinstance(item['data'], dict):\n                        for field in ['text', 'content', 'document']:\n                            if field in item['data'] and isinstance(item['data'][field], str):\n                                text = item['data'][field].strip()\n                                break\n                    \n                    if not text:\n                        continue\n                    \n                    # Tr√≠ch xu·∫•t t√¨nh c·∫£m v√† ch·ªß ƒë·ªÅ\n                    sentiment = \"Trung t√≠nh\"  # M·∫∑c ƒë·ªãnh\n                    topics = []  # Danh s√°ch ch·ªß ƒë·ªÅ\n                    \n                    if 'annotations' in item and isinstance(item['annotations'], list):\n                        for annotation in item['annotations']:\n                            if 'value' in annotation and isinstance(annotation['value'], dict):\n                                # T√¨m t√¨nh c·∫£m trong choices\n                                if 'choices' in annotation['value'] and isinstance(annotation['value']['choices'], list):\n                                    for choice in annotation['value']['choices']:\n                                        if choice in [\"T√≠ch c·ª±c\", \"Ti√™u c·ª±c\", \"Trung t√≠nh\"]:\n                                            sentiment = choice\n                                        else:\n                                            # N·∫øu kh√¥ng ph·∫£i t√¨nh c·∫£m, c√≥ th·ªÉ l√† ch·ªß ƒë·ªÅ\n                                            topics.append(choice)\n                                \n                                # T√¨m ch·ªß ƒë·ªÅ trong labels\n                                if 'labels' in annotation['value'] and isinstance(annotation['value']['labels'], list):\n                                    topics.extend(annotation['value']['labels'])\n                    \n                    # Th√™m v√†o danh s√°ch\n                    rows.append({\n                        'text': self.clean_text(text),\n                        'sentiment': sentiment,\n                        'topics': topics\n                    })\n            else:\n                # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng th√¥ng th∆∞·ªùng\n                for item in data if isinstance(data, list) else [data]:\n                    if isinstance(item, dict) and 'text' in item:\n                        topics = []\n                        if 'topics' in item and isinstance(item['topics'], list):\n                            topics = item['topics']\n                        elif 'labels' in item and isinstance(item['labels'], list):\n                            topics = item['labels']\n                        \n                        rows.append({\n                            'text': self.clean_text(item['text']),\n                            'sentiment': item.get('sentiment', \"Trung t√≠nh\"),\n                            'topics': topics\n                        })\n            \n            # T·∫°o DataFrame\n            df = pd.DataFrame(rows)\n            print(f\"ƒê√£ t·∫£i {len(df)} m·∫´u d·ªØ li·ªáu hu·∫•n luy·ªán\")\n            \n            # Ki·ªÉm tra ch·ªß ƒë·ªÅ\n            if 'topics' in df.columns:\n                all_topics = set()\n                for topics_list in df['topics']:\n                    if isinstance(topics_list, list):\n                        all_topics.update(topics_list)\n                print(f\"T√¨m th·∫•y {len(all_topics)} ch·ªß ƒë·ªÅ kh√°c nhau\")\n            \n            return df\n            \n        except Exception as e:\n            print(f\"L·ªói khi t·∫£i d·ªØ li·ªáu hu·∫•n luy·ªán: {str(e)}\")\n            return pd.DataFrame({'text': [], 'sentiment': [], 'topics': []})\n    \n    def clean_text(self, text):\n        \"\"\"L√†m s·∫°ch vƒÉn b·∫£n ti·∫øng Vi·ªát v·ªõi c√°c c·∫£i ti·∫øn cho ph√¢n lo·∫°i c·∫£m x√∫c\"\"\"\n        if not isinstance(text, str):\n            return \"\"\n        \n        # Chu·∫©n h√≥a Unicode (NFC)\n        text = unicodedata.normalize('NFC', text)\n        \n        # Thay th·∫ø emojis b·∫±ng m√¥ t·∫£\n        text = self._replace_emojis(text)\n        \n        # X·ª≠ l√Ω k√≠ t·ª± l·∫∑p (v√≠ d·ª•: \"qu√°aaaaa\" -> \"qu√°\")\n        text = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', text)\n        \n        # Chu·∫©n h√≥a d·∫•u c√¢u quan tr·ªçng cho c·∫£m x√∫c\n        text = re.sub(r'!{2,}', ' ! ! ', text)  # Chu·∫©n h√≥a nhi·ªÅu d·∫•u ch·∫•m than\n        text = re.sub(r'\\?{2,}', ' ? ? ', text)  # Chu·∫©n h√≥a nhi·ªÅu d·∫•u h·ªèi\n        \n        # Thay th·∫ø nhi·ªÅu kho·∫£ng tr·∫Øng b·∫±ng m·ªôt kho·∫£ng tr·∫Øng\n        text = re.sub(r'\\s+', ' ', text)\n        # Lo·∫°i b·ªè URL v√† th·∫ª HTML\n        text = re.sub(r'https?://\\S+|www\\.\\S+|<.*?>', '', text)\n        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n        return text.strip()\n    \n    def _replace_emojis(self, text):\n        \"\"\"Thay th·∫ø emojis ph·ªï bi·∫øn b·∫±ng t·ª´ m√¥ t·∫£ c·∫£m x√∫c\"\"\"\n        emoji_map = {\n            'üòä': ' vui_m·ª´ng ',\n            'üòÑ': ' c∆∞·ªùi_l·ªõn ',\n            'üòÉ': ' vui_v·∫ª ',\n            'üòÄ': ' c∆∞·ªùi ',\n            'üòç': ' y√™u_th√≠ch ',\n            'ü•∞': ' r·∫•t_y√™u_th√≠ch ',\n            'üòò': ' h√¥n_gi√≥ ',\n            '‚ù§Ô∏è': ' tr√°i_tim ',\n            '‚ô•Ô∏è': ' y√™u ',\n            'üëç': ' th√≠ch ',\n            'üëé': ' kh√¥ng_th√≠ch ',\n            'üò¢': ' bu·ªìn ',\n            'üò≠': ' kh√≥c ',\n            'üò°': ' t·ª©c_gi·∫≠n ',\n            'ü§¨': ' r·∫•t_t·ª©c_gi·∫≠n ',\n            'ü§î': ' suy_nghƒ© ',\n            'üò±': ' s·ª£_h√£i ',\n            'üôÑ': ' ng√°n_ng·∫©m ',\n            'ü§£': ' c∆∞·ªùi_lƒÉn ',\n            'üòÇ': ' c∆∞·ªùi_ra_n∆∞·ªõc_m·∫Øt '\n        }\n        \n        for emoji, replacement in emoji_map.items():\n            text = text.replace(emoji, replacement)\n        \n        return text\n    \n    def prepare_data(self, train_df):\n        \"\"\"Chu·∫©n b·ªã d·ªØ li·ªáu cho hu·∫•n luy·ªán v√† ki·ªÉm tra (s·ª≠ d·ª•ng split t·ª´ d·ªØ li·ªáu JSON)\"\"\"\n        if len(train_df) == 0:\n            print(\"C·∫£nh b√°o: DataFrame hu·∫•n luy·ªán r·ªóng, kh√¥ng th·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu\")\n            return None, None\n        \n        # T·∫°o √°nh x·∫° nh√£n t√¨nh c·∫£m\n        unique_sentiments = sorted(list(set(train_df['sentiment'])))\n        sentiment_to_idx = {sentiment: i for i, sentiment in enumerate(unique_sentiments)}\n        \n        # T·∫°o √°nh x·∫° nh√£n ch·ªß ƒë·ªÅ\n        all_topics = set()\n        if 'topics' in train_df.columns:\n            for topics_list in train_df['topics']:\n                if isinstance(topics_list, list):\n                    all_topics.update(topics_list)\n        \n        unique_topics = sorted(list(all_topics))\n        topic_to_idx = {topic: i for i, topic in enumerate(unique_topics)}\n        \n        # C√¢n b·∫±ng d·ªØ li·ªáu sentiment tr∆∞·ªõc khi chia\n        # ƒê·∫øm s·ªë l∆∞·ª£ng m·∫´u cho m·ªói nh√£n c·∫£m x√∫c\n        sentiment_counts = train_df['sentiment'].value_counts()\n        max_samples = sentiment_counts.max()\n        \n        # TƒÉng c∆∞·ªùng d·ªØ li·ªáu cho c√°c nh√≥m thi·ªÉu s·ªë\n        augmented_rows = []\n        for sentiment, count in sentiment_counts.items():\n            if count < max_samples:\n                # L·∫•y t·∫•t c·∫£ c√°c m·∫´u c·ªßa sentiment n√†y\n                sentiment_samples = train_df[train_df['sentiment'] == sentiment]\n                # S·ªë l∆∞·ª£ng m·∫´u c·∫ßn t·∫°o th√™m\n                num_to_generate = max_samples - count\n                \n                # T·∫°o th√™m m·∫´u b·∫±ng c√°ch nh√¢n b·∫£n v√† th√™m bi·∫øn th·ªÉ nh·∫π\n                for i in range(num_to_generate):\n                    # Ch·ªçn ng·∫´u nhi√™n m·ªôt m·∫´u ƒë·ªÉ nh√¢n b·∫£n\n                    sample = sentiment_samples.sample(1).iloc[0]\n                    \n                    # T·∫°o bi·∫øn th·ªÉ nh·∫π cho text\n                    original_text = sample['text']\n                    augmented_text = self._augment_text(original_text)\n                    \n                    # T·∫°o m·∫´u m·ªõi\n                    augmented_row = sample.copy()\n                    augmented_row['text'] = augmented_text\n                    \n                    augmented_rows.append(augmented_row)\n        \n        # Th√™m c√°c m·∫´u tƒÉng c∆∞·ªùng v√†o DataFrame\n        if augmented_rows:\n            augmented_df = pd.DataFrame(augmented_rows)\n            train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n            print(f\"ƒê√£ t·∫°o th√™m {len(augmented_rows)} m·∫´u tƒÉng c∆∞·ªùng ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu c·∫£m x√∫c\")\n        \n        # Chia d·ªØ li·ªáu hu·∫•n luy·ªán v√† validation\n        train_split_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n        print(f\"Chia d·ªØ li·ªáu: {len(train_split_df)} m·∫´u hu·∫•n luy·ªán, {len(val_df)} m·∫´u validation\")\n        \n        # Hu·∫•n luy·ªán tokenizer tr√™n t·∫≠p hu·∫•n luy·ªán\n        self.tokenizer.fit_on_texts(train_split_df['text'])\n        \n        # Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói s·ªë\n        train_sequences = self.tokenizer.texts_to_sequences(train_split_df['text'])\n        val_sequences = self.tokenizer.texts_to_sequences(val_df['text'])\n        \n        # ƒê·ªám chu·ªói ƒë·ªÉ c√≥ c√πng ƒë·ªô d√†i\n        train_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post')\n        val_padded = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post')\n        \n        # Chu·∫©n b·ªã nh√£n t√¨nh c·∫£m\n        train_sentiment_labels = self._prepare_sentiment_labels(train_split_df['sentiment'].tolist(), sentiment_to_idx)\n        val_sentiment_labels = self._prepare_sentiment_labels(val_df['sentiment'].tolist(), sentiment_to_idx)\n        \n        # Chu·∫©n b·ªã nh√£n ch·ªß ƒë·ªÅ (multi-label)\n        train_topic_labels = self._prepare_topic_labels(train_split_df['topics'].tolist(), topic_to_idx)\n        val_topic_labels = self._prepare_topic_labels(val_df['topics'].tolist(), topic_to_idx)\n        \n        # T·∫°o dictionary d·ªØ li·ªáu\n        data = {\n            'train': {\n                'sequences': train_padded,\n                'sentiment_labels': train_sentiment_labels,\n                'topic_labels': train_topic_labels\n            },\n            'val': {\n                'sequences': val_padded,\n                'sentiment_labels': val_sentiment_labels,\n                'topic_labels': val_topic_labels\n            }\n        }\n        \n        # T·∫°o √°nh x·∫° nh√£n v√† t·ª´ v·ª±ng\n        mappings = {\n            'unique_sentiments': unique_sentiments,\n            'sentiment_to_idx': sentiment_to_idx,\n            'idx_to_sentiment': {i: sentiment for sentiment, i in sentiment_to_idx.items()},\n            'num_sentiments': len(sentiment_to_idx),\n            'unique_topics': unique_topics,\n            'topic_to_idx': topic_to_idx,\n            'idx_to_topic': {i: topic for topic, i in topic_to_idx.items()},\n            'num_topics': len(topic_to_idx),\n            'word_index': self.tokenizer.word_index,\n            'vocab_size': min(VOCAB_SIZE, len(self.tokenizer.word_index) + 1)\n        }\n        \n        return data, mappings\n    \n    def _augment_text(self, text):\n        \"\"\"T·∫°o bi·∫øn th·ªÉ nh·∫π c·ªßa vƒÉn b·∫£n ƒë·ªÉ tƒÉng c∆∞·ªùng d·ªØ li·ªáu\"\"\"\n        if not isinstance(text, str) or len(text) < 10:\n            return text\n            \n        augmented = text\n        \n        # Danh s√°ch c√°c ph√©p bi·∫øn ƒë·ªïi\n        transforms = [\n            self._add_emphasis,\n            self._swap_words,\n            self._synonym_replacement,\n            self._remove_random_words,\n        ]\n        \n        # Ch·ªçn ng·∫´u nhi√™n 1-2 ph√©p bi·∫øn ƒë·ªïi\n        num_transforms = np.random.randint(1, 3)\n        selected_transforms = np.random.choice(transforms, num_transforms, replace=False)\n        \n        # √Åp d·ª•ng c√°c ph√©p bi·∫øn ƒë·ªïi\n        for transform in selected_transforms:\n            augmented = transform(augmented)\n            \n        return augmented\n    \n    def _add_emphasis(self, text):\n        \"\"\"Th√™m d·∫•u c√¢u nh·∫•n m·∫°nh c·∫£m x√∫c\"\"\"\n        # M·ªôt s·ªë t·ª´ quan tr·ªçng v√† emoji t∆∞∆°ng ·ª©ng\n        emphasis = [\n            (\"!\", \"!!\"),\n            (\"?\", \"??\"),\n            (\"r·∫•t\", \"r·∫•t r·∫•t\"),\n            (\"tuy·ªát\", \"tuy·ªát v·ªùi\"),\n            (\"t·ªët\", \"r·∫•t t·ªët\"),\n            (\"kh√¥ng t·ªët\", \"kh√¥ng t·ªët ch√∫t n√†o\"),\n            (\"t·ªá\", \"qu√° t·ªá\"),\n        ]\n        \n        result = text\n        # Ch·ªçn ng·∫´u nhi√™n 1 c√°ch nh·∫•n m·∫°nh ƒë·ªÉ √°p d·ª•ng\n        choice = np.random.choice(len(emphasis))\n        original, replacement = emphasis[choice]\n        \n        # Thay th·∫ø v·ªõi x√°c su·∫•t 0.7\n        if original in result and np.random.random() < 0.7:\n            result = result.replace(original, replacement, 1)\n            \n        return result\n    \n    def _swap_words(self, text):\n        \"\"\"Ho√°n ƒë·ªïi v·ªã tr√≠ c·ªßa hai t·ª´ c·∫°nh nhau\"\"\"\n        words = text.split()\n        if len(words) < 4:\n            return text\n            \n        # Ch·ªçn ng·∫´u nhi√™n v·ªã tr√≠ ƒë·ªÉ ho√°n ƒë·ªïi, tr√°nh t·ª´ ƒë·∫ßu v√† cu·ªëi\n        idx = np.random.randint(1, len(words) - 2)\n        \n        # Ho√°n ƒë·ªïi t·ª´\n        words[idx], words[idx + 1] = words[idx + 1], words[idx]\n        \n        return \" \".join(words)\n    \n    def _synonym_replacement(self, text):\n        \"\"\"Thay th·∫ø t·ª´ b·∫±ng t·ª´ ƒë·ªìng nghƒ©a\"\"\"\n        # M·ªôt s·ªë t·ª´ ƒë·ªìng nghƒ©a c∆° b·∫£n cho t√¨nh c·∫£m\n        synonyms = {\n            \"t·ªët\": [\"hay\", \"tuy·ªát\", \"t√≠ch c·ª±c\", \"kh√°\"],\n            \"tuy·ªát\": [\"tuy·ªát v·ªùi\", \"xu·∫•t s·∫Øc\", \"ho√†n h·∫£o\"],\n            \"t·ªá\": [\"k√©m\", \"t·ªìi\", \"kh√¥ng t·ªët\", \"d·ªü\"],\n            \"th√≠ch\": [\"y√™u th√≠ch\", \"∆∞a\", \"h√†i l√≤ng\"],\n            \"bu·ªìn\": [\"kh√¥ng vui\", \"th·∫•t v·ªçng\", \"ch√°n n·∫£n\"],\n            \"vui\": [\"vui v·∫ª\", \"h·∫°nh ph√∫c\", \"ph·∫•n kh·ªüi\"]\n        }\n        \n        words = text.split()\n        for i, word in enumerate(words):\n            if word in synonyms and np.random.random() < 0.3:\n                # Thay th·∫ø b·∫±ng t·ª´ ƒë·ªìng nghƒ©a ng·∫´u nhi√™n\n                replacement = np.random.choice(synonyms[word])\n                words[i] = replacement\n                break  # Ch·ªâ thay th·∫ø m·ªôt t·ª´\n                \n        return \" \".join(words)\n    \n    def _remove_random_words(self, text):\n        \"\"\"Lo·∫°i b·ªè m·ªôt t·ª´ ng·∫´u nhi√™n\"\"\"\n        words = text.split()\n        if len(words) < 5:\n            return text\n            \n        # Lo·∫°i b·ªè m·ªôt t·ª´ ng·∫´u nhi√™n kh√¥ng quan tr·ªçng (tr√°nh t·ª´ ƒë·∫ßu ti√™n)\n        idx = np.random.randint(1, len(words))\n        words.pop(idx)\n        \n        return \" \".join(words)\n    \n    def _prepare_sentiment_labels(self, sentiments, sentiment_to_idx):\n        \"\"\"Chu·∫©n b·ªã nh√£n t√¨nh c·∫£m (single-label)\"\"\"\n        labels = []\n        for sentiment in sentiments:\n            if sentiment in sentiment_to_idx:\n                idx = sentiment_to_idx[sentiment]\n            else:\n                # N·∫øu kh√¥ng t√¨m th·∫•y, s·ª≠ d·ª•ng Trung t√≠nh (ho·∫∑c ch·ªâ s·ªë th√≠ch h·ª£p)\n                idx = sentiment_to_idx.get(\"Trung t√≠nh\", 0)\n            labels.append(idx)\n        \n        return tf.keras.utils.to_categorical(labels, num_classes=len(sentiment_to_idx))\n    \n    def _prepare_topic_labels(self, topics_list, topic_to_idx):\n        \"\"\"Chu·∫©n b·ªã nh√£n ch·ªß ƒë·ªÅ (multi-label)\"\"\"\n        num_topics = len(topic_to_idx)\n        labels = np.zeros((len(topics_list), num_topics))\n        \n        for i, topics in enumerate(topics_list):\n            if isinstance(topics, list):\n                for topic in topics:\n                    if topic in topic_to_idx:\n                        labels[i, topic_to_idx[topic]] = 1\n        \n        return labels\n    \n    def build_model(self, mappings):\n        \"\"\"X√¢y d·ª±ng m√¥ h√¨nh BiLSTM cho ph√¢n lo·∫°i t√¨nh c·∫£m v√† ch·ªß ƒë·ªÅ\"\"\"\n        print(\"X√¢y d·ª±ng m√¥ h√¨nh BiLSTM k√©p cho ph√¢n lo·∫°i t√¨nh c·∫£m v√† ch·ªß ƒë·ªÅ...\")\n        \n        # L·∫•y k√≠ch th∆∞·ªõc t·ª´ v·ª±ng v√† s·ªë l·ªõp\n        vocab_size = mappings['vocab_size']\n        num_sentiments = mappings['num_sentiments']\n        num_topics = mappings['num_topics']\n        \n        # ƒê·∫ßu v√†o \n        input_layer = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_sequences')\n        \n        # Embedding layer\n        embedding = tf.keras.layers.Embedding(\n            vocab_size, EMBEDDING_DIM, input_length=MAX_LENGTH\n        )(input_layer)\n        \n        # Shared BiLSTM layers\n        lstm1 = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(128, return_sequences=True)\n        )(embedding)\n        lstm1_dropout = tf.keras.layers.Dropout(0.3)(lstm1)\n        \n        lstm2 = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(64, return_sequences=True)\n        )(lstm1_dropout)\n        lstm2_dropout = tf.keras.layers.Dropout(0.3)(lstm2)\n        \n        lstm3 = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(32)\n        )(lstm2_dropout)\n        lstm3_dropout = tf.keras.layers.Dropout(0.3)(lstm3)\n        \n        # Shared Dense layers\n        shared_dense1 = tf.keras.layers.Dense(256, activation='relu')(lstm3_dropout)\n        shared_dense1_dropout = tf.keras.layers.Dropout(0.3)(shared_dense1)\n        \n        shared_dense2 = tf.keras.layers.Dense(128, activation='relu')(shared_dense1_dropout)\n        shared_dense2_dropout = tf.keras.layers.Dropout(0.3)(shared_dense2)\n        \n        # Sentiment specific layers\n        sentiment_attention = SentimentAttention()(shared_dense2_dropout)\n        sentiment_dense = tf.keras.layers.Dense(64, activation='relu')(sentiment_attention[0])\n        sentiment_dropout = tf.keras.layers.Dropout(0.1)(sentiment_dense)\n        sentiment_output = tf.keras.layers.Dense(\n            num_sentiments, activation='softmax', name='sentiment_output'\n        )(sentiment_dropout)\n        \n        # Topic specific layers - th√™m nhi·ªÅu l·ªõp dense h∆°n cho t√°c v·ª• multi-label ph·ª©c t·∫°p\n        topic_dense1 = tf.keras.layers.Dense(128, activation='relu')(shared_dense2_dropout)\n        topic_dropout1 = tf.keras.layers.Dropout(0.2)(topic_dense1)\n        \n        topic_dense2 = tf.keras.layers.Dense(64, activation='relu')(topic_dropout1)\n        topic_dropout2 = tf.keras.layers.Dropout(0.2)(topic_dense2)\n        \n        topic_output = tf.keras.layers.Dense(\n            num_topics, activation='sigmoid', name='topic_output'\n        )(topic_dropout2)\n        \n        # T·∫°o m√¥ h√¨nh\n        model = tf.keras.Model(\n            inputs=input_layer,\n            outputs=[sentiment_output, topic_output]\n        )\n        \n        return model\n    \n    def train(self, data, mappings):\n        \"\"\"Hu·∫•n luy·ªán m√¥ h√¨nh\"\"\"\n        print(\"Hu·∫•n luy·ªán m√¥ h√¨nh...\")\n        \n        # T·∫°o m√¥ h√¨nh\n        model = self.build_model(mappings)\n        \n        # T√≠nh to√°n class weights cho sentiment\n        unique_sentiment_counts = Counter(np.argmax(data['train']['sentiment_labels'], axis=1))\n        total_sentiment_samples = sum(unique_sentiment_counts.values())\n        sentiment_class_weights = {\n            class_idx: total_sentiment_samples / (count * len(unique_sentiment_counts))\n            for class_idx, count in unique_sentiment_counts.items()\n        }\n        \n        print(\"T·ª∑ l·ªá c·∫£m x√∫c:\")\n        for class_idx, weight in sentiment_class_weights.items():\n            sentiment_name = mappings['idx_to_sentiment'][class_idx]\n            count = unique_sentiment_counts[class_idx]\n            print(f\"  - {sentiment_name}: {count}/{total_sentiment_samples} m·∫´u ({count/total_sentiment_samples*100:.2f}%) - weight: {weight:.2f}\")\n        \n        # Topic weights (s·ª≠ d·ª•ng positive_weights cho c√°c nh√£n hi·∫øm)\n        topic_sums = np.sum(data['train']['topic_labels'], axis=0)\n        total_samples = data['train']['topic_labels'].shape[0]\n        pos_weights = np.zeros(mappings['num_topics'])\n        \n        for i in range(mappings['num_topics']):\n            # C√¢n b·∫±ng t·ª∑ l·ªá √¢m/d∆∞∆°ng cho m·ªói ch·ªß ƒë·ªÅ\n            if topic_sums[i] > 0:\n                neg_samples = total_samples - topic_sums[i]\n                pos_weights[i] = neg_samples / (topic_sums[i] * 1.0)\n            else:\n                pos_weights[i] = 10.0  # gi√° tr·ªã m·∫∑c ƒë·ªãnh cao n·∫øu kh√¥ng c√≥ m·∫´u d∆∞∆°ng\n        \n        # Positive topic samples ƒë·ªÉ tƒÉng c∆∞·ªùng hu·∫•n luy·ªán\n        print(\"T·ª∑ l·ªá ch·ªß ƒë·ªÅ:\")\n        for i, topic in enumerate(mappings['unique_topics']):\n            pos_count = topic_sums[i] \n            print(f\"  - {topic}: {pos_count}/{total_samples} m·∫´u ({pos_count/total_samples*100:.2f}%) - weight: {pos_weights[i]:.2f}\")\n        \n        # TƒÉng c∆∞·ªùng d·ªØ li·ªáu c√¢n b·∫±ng cho sentiment\n        X_train = data['train']['sequences']\n        y_train_sentiment = data['train']['sentiment_labels']\n        y_train_topic = data['train']['topic_labels']\n        \n        # X√°c ƒë·ªãnh l·ªõp thi·ªÉu s·ªë v√† ƒëa s·ªë\n        minority_class = min(unique_sentiment_counts, key=unique_sentiment_counts.get)\n        majority_classes = [i for i in unique_sentiment_counts.keys() if i != minority_class]\n        \n        # Th√™m focal loss cho sentiment ƒë·ªÉ t·∫≠p trung v√†o c√°c m·∫´u kh√≥ ph√¢n lo·∫°i\n        def focal_loss(gamma=2.0, alpha=0.25):\n            def focal_loss_fn(y_true, y_pred):\n                # Clip values ƒë·ªÉ tr√°nh log(0)\n                y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n                \n                # Focal loss formula\n                cross_entropy = -y_true * tf.math.log(y_pred)\n                weight = tf.pow(1 - y_pred, gamma) * y_true\n                \n                # Apply weights t·ª´ class weights\n                weights = np.ones(mappings['num_sentiments'])\n                for i in range(mappings['num_sentiments']):\n                    if i in sentiment_class_weights:\n                        weights[i] = sentiment_class_weights[i]\n                \n                weight_tensor = tf.constant(weights, dtype=tf.float32)\n                weighted_focal_loss = cross_entropy * weight * tf.expand_dims(weight_tensor, 0)\n                \n                return tf.reduce_sum(weighted_focal_loss, axis=-1)\n            return focal_loss_fn\n        \n        # Weighted binary crossentropy cho topic v·ªõi positive weights\n        def weighted_binary_crossentropy(y_true, y_pred):\n            # √Åp d·ª•ng positive_weights cho t·ª´ng ch·ªß ƒë·ªÅ\n            weights = tf.constant(pos_weights, dtype=tf.float32)\n            y_true = tf.cast(y_true, tf.float32)\n            \n            # Tr√°nh log(0)\n            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n            \n            # C√¥ng th·ª©c weighted binary crossentropy\n            pos_term = -y_true * tf.math.log(y_pred) * weights  # positive samples c√≥ tr·ªçng s·ªë cao h∆°n\n            neg_term = -(1 - y_true) * tf.math.log(1 - y_pred)\n            \n            return tf.reduce_mean(pos_term + neg_term, axis=-1)\n        \n        # Bi√™n d·ªãch l·∫°i m√¥ h√¨nh v·ªõi c√°c tr·ªçng s·ªë\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n            loss={\n                'sentiment_output': focal_loss(gamma=2.0),  # D√πng focal loss thay v√¨ weighted categorical\n                'topic_output': weighted_binary_crossentropy\n            },\n            loss_weights={\n                'sentiment_output': 1.5,  # TƒÉng tr·ªçng s·ªë cho sentiment\n                'topic_output': 1.0\n            },\n            metrics={\n                'sentiment_output': ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n                'topic_output': [tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n            }\n        )\n        \n        # Callbacks\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_sentiment_output_accuracy',\n                patience=10,  # TƒÉng patience ƒë·ªÉ cho m√¥ h√¨nh th·ªëi gian h·ªôi t·ª•\n                restore_best_weights=True,\n                mode='max'\n            ),\n            tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_sentiment_output_accuracy',\n                factor=0.5,\n                patience=3,\n                min_lr=1e-6,\n                mode='max'\n            ),\n            # Th√™m checkpoint ƒë·ªÉ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t theo sentiment accuracy\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath='best_sentiment_model.keras',\n                save_best_only=True,\n                monitor='val_sentiment_output_accuracy',\n                mode='max'\n            )\n        ]\n        \n        try:\n            # Hu·∫•n luy·ªán\n            history = model.fit(\n                data['train']['sequences'],\n                {\n                    'sentiment_output': data['train']['sentiment_labels'],\n                    'topic_output': data['train']['topic_labels']\n                },\n                validation_data=(\n                    data['val']['sequences'],\n                    {\n                        'sentiment_output': data['val']['sentiment_labels'],\n                        'topic_output': data['val']['topic_labels']\n                    }\n                ),\n                epochs=EPOCHS,\n                batch_size=BATCH_SIZE,\n                callbacks=callbacks\n                # ƒê√£ lo·∫°i b·ªè class_weight v√¨ kh√¥ng h·ªó tr·ª£ cho m√¥ h√¨nh ƒëa ƒë·∫ßu ra\n            )\n            \n            # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán ƒë·ªÉ ph√¢n t√≠ch sau\n            self.history = history.history\n            \n            # L∆∞u m√¥ h√¨nh\n            self.save_model(model, mappings)\n            \n            return model\n        except Exception as e:\n            print(f\"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {str(e)}\")\n            return None\n    \n    def predict_sentiment(self, model, text, mappings):\n        \"\"\"D·ª± ƒëo√°n t√¨nh c·∫£m cho vƒÉn b·∫£n v·ªõi c√°c c·∫£i ti·∫øn\"\"\"\n        if model is None or mappings is None:\n            print(\"Kh√¥ng c√≥ m√¥ h√¨nh ho·∫∑c √°nh x·∫° nh√£n\")\n            return \"Kh√¥ng x√°c ƒë·ªãnh\", 0.0\n        \n        # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n        cleaned_text = self.clean_text(text)\n        sequence = self.tokenizer.texts_to_sequences([cleaned_text])\n        padded = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n        \n        # T·∫°o nhi·ªÅu bi·∫øn th·ªÉ c·ªßa m·ªôt c√¢u ƒë·ªÉ tƒÉng c∆∞·ªùng hi·ªáu su·∫•t d·ª± ƒëo√°n\n        # V√≠ d·ª•: th√™m d·∫•u c√¢u bi·ªÉu th·ªã c·∫£m x√∫c, thay ƒë·ªïi t·ª´ ng·ªØ nh·∫π, v.v.\n        variations = [\n            padded,  # C√¢u g·ªëc\n        ]\n        \n        # D·ª± ƒëo√°n\n        try:\n            # D·ª± ƒëo√°n t·∫•t c·∫£ c√°c bi·∫øn th·ªÉ\n            all_predictions = []\n            \n            for var in variations:\n                predictions = model.predict(var)\n                all_predictions.append(predictions[0][0])  # L·∫•y k·∫øt qu·∫£ sentiment\n            \n            # L·∫•y trung b√¨nh c·ªßa t·∫•t c·∫£ c√°c d·ª± ƒëo√°n\n            avg_predictions = np.mean(all_predictions, axis=0)\n            \n            # X·ª≠ l√Ω k·∫øt qu·∫£ t√¨nh c·∫£m\n            sentiment_idx = np.argmax(avg_predictions)\n            sentiment = mappings['idx_to_sentiment'][sentiment_idx]\n            confidence = float(avg_predictions[sentiment_idx])\n            \n            # Th√™m logic ƒë·ªÉ x√°c ƒë·ªãnh m·ª©c ƒë·ªô ch·∫Øc ch·∫Øn\n            confidence_level = \"cao\" if confidence > 0.8 else \"trung b√¨nh\" if confidence > 0.6 else \"th·∫•p\"\n            \n            return sentiment, confidence, confidence_level\n        except Exception as e:\n            print(f\"L·ªói khi d·ª± ƒëo√°n: {str(e)}\")\n            return \"Kh√¥ng x√°c ƒë·ªãnh\", 0.0, \"kh√¥ng x√°c ƒë·ªãnh\"\n    \n    def predict_topics(self, model, text, mappings, threshold=None, max_topics=5):\n        \"\"\"D·ª± ƒëo√°n ch·ªß ƒë·ªÅ cho vƒÉn b·∫£n\n        \n        Args:\n            model: M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n            text: VƒÉn b·∫£n c·∫ßn d·ª± ƒëo√°n\n            mappings: B·∫£n ƒë·ªì √°nh x·∫° nh√£n\n            threshold: Ng∆∞·ª°ng x√°c su·∫•t ƒë·ªÉ ch·ªçn ch·ªß ƒë·ªÅ (m·∫∑c ƒë·ªãnh: 0.2)\n            max_topics: S·ªë l∆∞·ª£ng ch·ªß ƒë·ªÅ t·ªëi ƒëa tr·∫£ v·ªÅ (m·∫∑c ƒë·ªãnh: 5)\n            \n        Returns:\n            Tuple (selected_topics, confidences): Danh s√°ch ch·ªß ƒë·ªÅ v√† ƒë·ªô tin c·∫≠y t∆∞∆°ng ·ª©ng\n        \"\"\"\n        if model is None or mappings is None:\n            print(\"Kh√¥ng c√≥ m√¥ h√¨nh ho·∫∑c √°nh x·∫° nh√£n\")\n            return [], []\n        \n        if threshold is None:\n            threshold = self.topic_threshold\n            \n        # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n        cleaned_text = self.clean_text(text)\n        sequence = self.tokenizer.texts_to_sequences([cleaned_text])\n        padded = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n        \n        # D·ª± ƒëo√°n\n        try:\n            predictions = model.predict(padded)\n            \n            # X·ª≠ l√Ω k·∫øt qu·∫£ ch·ªß ƒë·ªÅ (l·∫•y ƒë·∫ßu ra th·ª© hai l√† topics)\n            topic_preds = predictions[1][0]  # L·∫•y d·ª± ƒëo√°n ch·ªß ƒë·ªÅ\n            \n            # T·∫°o danh s√°ch c√°c ch·ªß ƒë·ªÅ v·ªõi x√°c su·∫•t\n            topics_with_probs = [(mappings['idx_to_topic'][i], float(prob)) \n                             for i, prob in enumerate(topic_preds) \n                             if prob >= threshold]\n            \n            # S·∫Øp x·∫øp theo ƒë·ªô tin c·∫≠y gi·∫£m d·∫ßn v√† ch·ªâ l·∫•y t·ªëi ƒëa max_topics ch·ªß ƒë·ªÅ\n            topics_with_probs.sort(key=lambda x: x[1], reverse=True)\n            topics_with_probs = topics_with_probs[:max_topics]\n            \n            # T√°ch th√†nh hai danh s√°ch ri√™ng bi·ªát\n            if topics_with_probs:\n                selected_topics, confidences = zip(*topics_with_probs)\n                return list(selected_topics), list(confidences)\n            else:\n                return [], []\n                \n        except Exception as e:\n            print(f\"L·ªói khi d·ª± ƒëo√°n ch·ªß ƒë·ªÅ: {str(e)}\")\n            return [], []\n            \n    def predict_batch(self, model, texts, mappings, topic_threshold=None, max_topics=5):\n        \"\"\"D·ª± ƒëo√°n t√¨nh c·∫£m v√† ch·ªß ƒë·ªÅ cho m·ªôt lo·∫°t vƒÉn b·∫£n\n        \n        Args:\n            model: M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n            texts: Danh s√°ch c√°c vƒÉn b·∫£n c·∫ßn d·ª± ƒëo√°n\n            mappings: B·∫£n ƒë·ªì √°nh x·∫° nh√£n\n            topic_threshold: Ng∆∞·ª°ng x√°c su·∫•t ƒë·ªÉ ch·ªçn ch·ªß ƒë·ªÅ (m·∫∑c ƒë·ªãnh: 0.2)\n            max_topics: S·ªë l∆∞·ª£ng ch·ªß ƒë·ªÅ t·ªëi ƒëa tr·∫£ v·ªÅ (m·∫∑c ƒë·ªãnh: 5)\n            \n        Returns:\n            Danh s√°ch k·∫øt qu·∫£ d·ª± ƒëo√°n, m·ªói k·∫øt qu·∫£ bao g·ªìm text, sentiment, sentiment_confidence, topics, topic_confidences\n        \"\"\"\n        if model is None or mappings is None:\n            print(\"Kh√¥ng c√≥ m√¥ h√¨nh ho·∫∑c √°nh x·∫° nh√£n\")\n            return []\n        \n        if topic_threshold is None:\n            topic_threshold = self.topic_threshold\n            \n        # L√†m s·∫°ch v√† ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n        cleaned_texts = [self.clean_text(text) for text in texts]\n        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)\n        padded = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post')\n        \n        # D·ª± ƒëo√°n\n        try:\n            predictions = model.predict(padded)\n            sentiment_preds = predictions[0]  # [batch_size, num_sentiments]\n            topic_preds = predictions[1]  # [batch_size, num_topics]\n            \n            # X·ª≠ l√Ω k·∫øt qu·∫£\n            results = []\n            for i, (sent_pred, topic_pred) in enumerate(zip(sentiment_preds, topic_preds)):\n                # Sentiment\n                sentiment_idx = np.argmax(sent_pred)\n                sentiment = mappings['idx_to_sentiment'][sentiment_idx]\n                sentiment_confidence = float(sent_pred[sentiment_idx])\n                confidence_level = \"cao\" if sentiment_confidence > 0.8 else \"trung b√¨nh\" if sentiment_confidence > 0.6 else \"th·∫•p\"\n                \n                # Topics - t·∫°o danh s√°ch v·ªõi x√°c su·∫•t\n                topics_with_probs = [(mappings['idx_to_topic'][j], float(prob)) \n                                 for j, prob in enumerate(topic_pred) \n                                 if prob >= topic_threshold]\n                \n                # S·∫Øp x·∫øp theo ƒë·ªô tin c·∫≠y gi·∫£m d·∫ßn v√† ch·ªâ l·∫•y t·ªëi ƒëa max_topics ch·ªß ƒë·ªÅ\n                topics_with_probs.sort(key=lambda x: x[1], reverse=True)\n                topics_with_probs = topics_with_probs[:max_topics]\n                \n                # T√°ch th√†nh hai danh s√°ch ri√™ng bi·ªát\n                if topics_with_probs:\n                    selected_topics, topic_confidences = zip(*topics_with_probs)\n                    selected_topics = list(selected_topics)\n                    topic_confidences = list(topic_confidences)\n                else:\n                    selected_topics, topic_confidences = [], []\n                \n                results.append({\n                    'text': texts[i],\n                    'sentiment': sentiment,\n                    'sentiment_confidence': sentiment_confidence,\n                    'confidence_level': confidence_level,\n                    'topics': selected_topics,\n                    'topic_confidences': topic_confidences\n                })\n            \n            return results\n        except Exception as e:\n            print(f\"L·ªói khi d·ª± ƒëo√°n h√†ng lo·∫°t: {str(e)}\")\n            return []\n    \n    def evaluate(self, model, data, mappings):\n        \"\"\"ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validation v·ªõi t·∫≠p trung v√†o ƒë·ªô ch√≠nh x√°c c·∫£m x√∫c\"\"\"\n        if model is None or data is None or mappings is None:\n            print(\"Kh√¥ng c√≥ m√¥ h√¨nh, d·ªØ li·ªáu ho·∫∑c √°nh x·∫° nh√£n ƒë·ªÉ ƒë√°nh gi√°\")\n            return None\n        \n        try:\n            # ƒê√°nh gi√°\n            print(\"ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validation...\")\n            results = model.evaluate(\n                data['val']['sequences'],\n                {\n                    'sentiment_output': data['val']['sentiment_labels'],\n                    'topic_output': data['val']['topic_labels']\n                }\n            )\n            \n            # In k·∫øt qu·∫£\n            metrics = ['loss', 'sentiment_output_loss', 'topic_output_loss', \n                       'sentiment_output_accuracy', 'topic_output_auc', \n                       'sentiment_output_precision', 'sentiment_output_recall', \n                       'topic_output_precision', 'topic_output_recall']\n            for i, metric in enumerate(metrics):\n                if i < len(results):\n                    print(f\"{metric}: {results[i]:.4f}\")\n            \n            # Ph√¢n t√≠ch chi ti·∫øt h∆°n cho sentiment\n            print(\"\\nƒê√°nh gi√° chi ti·∫øt sentiment:\")\n            \n            # L·∫•y d·ª± ƒëo√°n\n            predictions = model.predict(data['val']['sequences'])\n            sentiment_preds = predictions[0]\n            \n            # Chuy·ªÉn ƒë·ªïi v·ªÅ nh√£n s·ªë\n            true_labels = np.argmax(data['val']['sentiment_labels'], axis=1)\n            pred_labels = np.argmax(sentiment_preds, axis=1)\n            \n            # T√≠nh confusion matrix\n            from sklearn.metrics import confusion_matrix, classification_report\n            \n            cm = confusion_matrix(true_labels, pred_labels)\n            print(\"Confusion Matrix:\")\n            sentiment_names = [mappings['idx_to_sentiment'][i] for i in range(mappings['num_sentiments'])]\n            \n            # In confusion matrix d·ªÖ ƒë·ªçc\n            print(\"Th·ª±c t·∫ø \\\\ D·ª± ƒëo√°n:\")\n            header = \"    \" + \"\".join([f\"{name[:8]:12s}\" for name in sentiment_names])\n            print(header)\n            \n            for i, row in enumerate(cm):\n                print(f\"{sentiment_names[i][:8]:8s} {' '.join([f'{x:11d}' for x in row])}\")\n            \n            # In classification report\n            print(\"\\nClassification Report:\")\n            report = classification_report(\n                true_labels, pred_labels,\n                target_names=sentiment_names,\n                digits=4\n            )\n            print(report)\n            \n            return results\n        except Exception as e:\n            print(f\"L·ªói khi ƒë√°nh gi√° m√¥ h√¨nh: {str(e)}\")\n            return None\n    \n    def save_model(self, model, mappings):\n        \"\"\"L∆∞u m√¥ h√¨nh v√† √°nh x·∫° nh√£n\"\"\"\n        model_save_path = \"vietnamese_sentiment_model.keras\"  \n        model.save(model_save_path)\n        \n        # L∆∞u mappings ƒë·ªÉ s·ª≠ d·ª•ng khi t·∫£i m√¥ h√¨nh\n        mappings_path = \"vietnamese_sentiment_mappings.json\"\n        with open(mappings_path, 'w', encoding='utf-8') as f:\n            # Chuy·ªÉn ƒë·ªïi c√°c key t·ª´ int sang str ƒë·ªÉ c√≥ th·ªÉ l∆∞u th√†nh JSON\n            serializable_mappings = {\n                'unique_sentiments': mappings['unique_sentiments'],\n                'sentiment_to_idx': mappings['sentiment_to_idx'],\n                'idx_to_sentiment': {str(i): s for i, s in mappings['idx_to_sentiment'].items()},\n                'num_sentiments': mappings['num_sentiments'],\n                'unique_topics': mappings['unique_topics'],\n                'topic_to_idx': mappings['topic_to_idx'],\n                'idx_to_topic': {str(i): t for i, t in mappings['idx_to_topic'].items()},\n                'num_topics': mappings['num_topics'],\n                'vocab_size': mappings['vocab_size']\n            }\n            json.dump(serializable_mappings, f, ensure_ascii=False, indent=2)\n        \n        print(f\"ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i {model_save_path}\")\n        print(f\"ƒê√£ l∆∞u √°nh x·∫° nh√£n t·∫°i {mappings_path}\")\n        \n    def load_model(self, model_path=None, mappings_path=None):\n        \"\"\"T·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\"\"\"\n        # ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh\n        if model_path is None:\n            model_path = \"vietnamese_sentiment_model.keras\"\n        if mappings_path is None:\n            mappings_path = \"vietnamese_sentiment_mappings.json\"\n            \n        try:\n            # Ki·ªÉm tra t·∫≠p tin t·ªìn t·∫°i\n            if not os.path.exists(model_path):\n                print(f\"Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh t·∫°i {model_path}\")\n                return None, None\n                \n            if not os.path.exists(mappings_path):\n                print(f\"Kh√¥ng t√¨m th·∫•y file √°nh x·∫° nh√£n t·∫°i {mappings_path}\")\n                return None, None\n            \n            # T·∫£i mappings\n            with open(mappings_path, 'r', encoding='utf-8') as f:\n                loaded_mappings = json.load(f)\n                \n            # Chuy·ªÉn ƒë·ªïi key t·ª´ str v·ªÅ int\n            if 'idx_to_sentiment' in loaded_mappings:\n                loaded_mappings['idx_to_sentiment'] = {int(i): s for i, s in loaded_mappings['idx_to_sentiment'].items()}\n            if 'idx_to_topic' in loaded_mappings:\n                loaded_mappings['idx_to_topic'] = {int(i): t for i, t in loaded_mappings['idx_to_topic'].items()}\n            \n            # T·∫°o c√°c h√†m loss t√πy ch·ªânh tr·ªëng ƒë·ªÉ t·∫£i m√¥ h√¨nh\n            # (ch√∫ng s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t l·∫°i khi hu·∫•n luy·ªán)\n            def custom_sentiment_loss(y_true, y_pred):\n                return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n                \n            def custom_topic_loss(y_true, y_pred):\n                return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n                \n            # T·∫£i m√¥ h√¨nh\n            model = tf.keras.models.load_model(\n                model_path,\n                custom_objects={\n                    'weighted_categorical_crossentropy': custom_sentiment_loss,\n                    'weighted_binary_crossentropy': custom_topic_loss\n                }\n            )\n            \n            print(f\"ƒê√£ t·∫£i m√¥ h√¨nh t·ª´ {model_path}\")\n            return model, loaded_mappings\n            \n        except Exception as e:\n            print(f\"L·ªói khi t·∫£i m√¥ h√¨nh: {str(e)}\")\n            return None, None\n\n# V√≠ d·ª• s·ª≠ d·ª•ng\nif __name__ == \"__main__\":\n    try:\n        # Kh·ªüi t·∫°o classifier\n        classifier = VietnameseTextClassifier()\n        \n        # T·∫£i d·ªØ li·ªáu hu·∫•n luy·ªán JSON\n        json_path = \"/kaggle/input/datata/train data.json\"  # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON\n        \n        # Ki·ªÉm tra file JSON t·ªìn t·∫°i\n        if not os.path.exists(json_path):\n            print(f\"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i {json_path}\")\n            # Th·ª≠ t·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n            model, mappings = classifier.load_model()\n        else:\n            train_df = classifier.load_train_data(json_path)\n            \n            # Chu·∫©n b·ªã d·ªØ li·ªáu (t·ª± ƒë·ªông t·∫°o t·∫≠p validation t·ª´ d·ªØ li·ªáu hu·∫•n luy·ªán)\n            data, mappings = classifier.prepare_data(train_df)\n            \n            # Hu·∫•n luy·ªán m√¥ h√¨nh\n            model = classifier.train(data, mappings)\n            \n            # ƒê√°nh gi√° m√¥ h√¨nh\n            if model is not None:\n                classifier.evaluate(model, data, mappings)\n        \n        # V√≠ d·ª• d·ª± ƒëo√°n\n        example_texts = [\n            \"T√¥i r·∫•t th√≠ch s·∫£n ph·∫©m n√†y, ch·∫•t l∆∞·ª£ng tuy·ªát v·ªùi!\",\n            \"D·ªãch v·ª• t·∫°i c·ª≠a h√†ng n√†y th·∫≠t t·ªá.\",\n            \"S·∫£n ph·∫©m ƒë√∫ng nh∆∞ m√¥ t·∫£, giao h√†ng ƒë√∫ng h·∫πn.\"\n        ]\n        \n        # D·ª± ƒëo√°n d√πng m√¥ h√¨nh hi·ªán t·∫°i\n        if model is not None and mappings is not None:\n            print(\"\\nD·ª± ƒëo√°n ƒë∆°n:\")\n            for text in example_texts:\n                sentiment, confidence, confidence_level = classifier.predict_sentiment(model, text, mappings)\n                topics, topic_conf = classifier.predict_topics(model, text, mappings)\n                \n                print(f\"VƒÉn b·∫£n: {text}\")\n                print(f\"T√¨nh c·∫£m: {sentiment}, ƒê·ªô tin c·∫≠y: {confidence:.4f}, M·ª©c ƒë·ªô tin c·∫≠y: {confidence_level}\")\n                print(f\"Ch·ªß ƒë·ªÅ: {topics}\")\n                print(f\"ƒê·ªô tin c·∫≠y ch·ªß ƒë·ªÅ: {[f'{conf:.4f}' for conf in topic_conf]}\")\n                print(\"-\" * 50)\n            \n            print(\"\\nD·ª± ƒëo√°n h√†ng lo·∫°t:\")\n            batch_results = classifier.predict_batch(model, example_texts, mappings)\n            for result in batch_results:\n                print(f\"VƒÉn b·∫£n: {result['text']}\")\n                print(f\"T√¨nh c·∫£m: {result['sentiment']}, ƒê·ªô tin c·∫≠y: {result['sentiment_confidence']:.4f}, M·ª©c ƒë·ªô: {result['confidence_level']}\")\n                print(f\"Ch·ªß ƒë·ªÅ: {result['topics']}\")\n                print(f\"ƒê·ªô tin c·∫≠y ch·ªß ƒë·ªÅ: {[f'{conf:.4f}' for conf in result['topic_confidences']]}\")\n                print(\"-\" * 50)\n        else:\n            print(\"Kh√¥ng th·ªÉ d·ª± ƒëo√°n do kh√¥ng c√≥ m√¥ h√¨nh ho·∫∑c √°nh x·∫° nh√£n\")\n        \n    except Exception as e:\n        print(f\"L·ªói: {str(e)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T10:14:59.158371Z","iopub.execute_input":"2025-03-25T10:14:59.158720Z","iopub.status.idle":"2025-03-25T10:15:53.452477Z","shell.execute_reply.started":"2025-03-25T10:14:59.158694Z","shell.execute_reply":"2025-03-25T10:15:53.451665Z"}},"outputs":[{"name":"stdout","text":"Kh·ªüi t·∫°o Vietnamese Text Classifier v·ªõi m√¥ h√¨nh BiLSTM...\nƒêang t·∫£i d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ /kaggle/input/datata/train data.json...\nC·∫•u tr√∫c d·ªØ li·ªáu: <class 'list'>\nS·ªë l∆∞·ª£ng m·ª•c: 1696\nPh√°t hi·ªán ƒë·ªãnh d·∫°ng Label Studio\nƒê√£ t·∫£i 1695 m·∫´u d·ªØ li·ªáu hu·∫•n luy·ªán\nT√¨m th·∫•y 23 ch·ªß ƒë·ªÅ kh√°c nhau\nƒê√£ t·∫°o th√™m 1197 m·∫´u tƒÉng c∆∞·ªùng ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu c·∫£m x√∫c\nChia d·ªØ li·ªáu: 2313 m·∫´u hu·∫•n luy·ªán, 579 m·∫´u validation\nHu·∫•n luy·ªán m√¥ h√¨nh...\nX√¢y d·ª±ng m√¥ h√¨nh BiLSTM k√©p cho ph√¢n lo·∫°i t√¨nh c·∫£m v√† ch·ªß ƒë·ªÅ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"T·ª∑ l·ªá c·∫£m x√∫c:\n  - Trung t√≠nh: 776/2313 m·∫´u (33.55%) - weight: 0.99\n  - Ti√™u c·ª±c: 763/2313 m·∫´u (32.99%) - weight: 1.01\n  - T√≠ch c·ª±c: 774/2313 m·∫´u (33.46%) - weight: 1.00\nT·ª∑ l·ªá ch·ªß ƒë·ªÅ:\n  - B·∫•t ƒë·ªông s·∫£n: 86.0/2313 m·∫´u (3.72%) - weight: 25.90\n  - Ch√≠nh tr·ªã: 84.0/2313 m·∫´u (3.63%) - weight: 26.54\n  - Ch·ª©ng kho√°n: 43.0/2313 m·∫´u (1.86%) - weight: 52.79\n  - Covid-19: 292.0/2313 m·∫´u (12.62%) - weight: 6.92\n  - C√¥ng ngh·ªá: 154.0/2313 m·∫´u (6.66%) - weight: 14.02\n  - Du l·ªãch: 56.0/2313 m·∫´u (2.42%) - weight: 40.30\n  - Game: 24.0/2313 m·∫´u (1.04%) - weight: 95.38\n  - Giao th√¥ng: 44.0/2313 m·∫´u (1.90%) - weight: 51.57\n  - Gi√°o d·ª•c: 61.0/2313 m·∫´u (2.64%) - weight: 36.92\n  - Gi·∫£i tr√≠: 292.0/2313 m·∫´u (12.62%) - weight: 6.92\n  - H√≥ng bi·∫øn: 257.0/2313 m·∫´u (11.11%) - weight: 8.00\n  - Khoa h·ªçc: 33.0/2313 m·∫´u (1.43%) - weight: 69.09\n  - Kh√¥ng x√°c ƒë·ªãnh: 24.0/2313 m·∫´u (1.04%) - weight: 95.38\n  - Kinh t·∫ø: 405.0/2313 m·∫´u (17.51%) - weight: 4.71\n  - M√¥i tr∆∞·ªùng: 102.0/2313 m·∫´u (4.41%) - weight: 21.68\n  - Phim ·∫£nh: 54.0/2313 m·∫´u (2.33%) - weight: 41.83\n  - Ph√°p lu·∫≠t: 172.0/2313 m·∫´u (7.44%) - weight: 12.45\n  - S·ª©c kho·∫ª: 209.0/2313 m·∫´u (9.04%) - weight: 10.07\n  - Th·∫ø gi·ªõi: 287.0/2313 m·∫´u (12.41%) - weight: 7.06\n  - Th·ªÉ thao: 188.0/2313 m·∫´u (8.13%) - weight: 11.30\n  - VƒÉn ho√°: 116.0/2313 m·∫´u (5.02%) - weight: 18.94\n  - X√£ h·ªôi: 394.0/2313 m·∫´u (17.03%) - weight: 4.87\n  - ƒê·ªùi s·ªëng: 684.0/2313 m·∫´u (29.57%) - weight: 2.38\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['attention_weight', 'attention_bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - loss: 2.0238 - sentiment_output_accuracy: 0.3341 - sentiment_output_loss: 0.7348 - sentiment_output_precision: 0.0000e+00 - sentiment_output_recall: 0.0000e+00 - topic_output_auc: 0.5098 - topic_output_loss: 1.2891 - topic_output_precision_1: 0.0795 - topic_output_recall_1: 0.3941 - val_loss: 2.0015 - val_sentiment_output_accuracy: 0.5492 - val_sentiment_output_loss: 0.6998 - val_sentiment_output_precision: 0.0000e+00 - val_sentiment_output_recall: 0.0000e+00 - val_topic_output_auc: 0.4919 - val_topic_output_loss: 1.2826 - val_topic_output_precision_1: 0.0813 - val_topic_output_recall_1: 0.3691 - learning_rate: 0.0010\nEpoch 2/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.8208 - sentiment_output_accuracy: 0.5937 - sentiment_output_loss: 0.5440 - sentiment_output_precision: 0.6437 - sentiment_output_recall: 0.2484 - topic_output_auc: 0.5425 - topic_output_loss: 1.2767 - topic_output_precision_1: 0.0837 - topic_output_recall_1: 0.4392 - val_loss: 1.6103 - val_sentiment_output_accuracy: 0.7168 - val_sentiment_output_loss: 0.3331 - val_sentiment_output_precision: 0.7995 - val_sentiment_output_recall: 0.5164 - val_topic_output_auc: 0.5910 - val_topic_output_loss: 1.2465 - val_topic_output_precision_1: 0.0927 - val_topic_output_recall_1: 0.4928 - learning_rate: 0.0010\nEpoch 3/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.4898 - sentiment_output_accuracy: 0.7680 - sentiment_output_loss: 0.2458 - sentiment_output_precision: 0.7967 - sentiment_output_recall: 0.6839 - topic_output_auc: 0.5889 - topic_output_loss: 1.2440 - topic_output_precision_1: 0.0932 - topic_output_recall_1: 0.5023 - val_loss: 1.4847 - val_sentiment_output_accuracy: 0.7858 - val_sentiment_output_loss: 0.2276 - val_sentiment_output_precision: 0.8086 - val_sentiment_output_recall: 0.7444 - val_topic_output_auc: 0.5847 - val_topic_output_loss: 1.2298 - val_topic_output_precision_1: 0.0887 - val_topic_output_recall_1: 0.5169 - learning_rate: 0.0010\nEpoch 4/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 1.3727 - sentiment_output_accuracy: 0.8788 - sentiment_output_loss: 0.1510 - sentiment_output_precision: 0.8861 - sentiment_output_recall: 0.8630 - topic_output_auc: 0.5897 - topic_output_loss: 1.2216 - topic_output_precision_1: 0.0921 - topic_output_recall_1: 0.5236 - val_loss: 1.4831 - val_sentiment_output_accuracy: 0.8117 - val_sentiment_output_loss: 0.2372 - val_sentiment_output_precision: 0.8209 - val_sentiment_output_recall: 0.7997 - val_topic_output_auc: 0.6198 - val_topic_output_loss: 1.2178 - val_topic_output_precision_1: 0.1029 - val_topic_output_recall_1: 0.5227 - learning_rate: 0.0010\nEpoch 5/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.2913 - sentiment_output_accuracy: 0.9360 - sentiment_output_loss: 0.0796 - sentiment_output_precision: 0.9418 - sentiment_output_recall: 0.9297 - topic_output_auc: 0.6049 - topic_output_loss: 1.2117 - topic_output_precision_1: 0.0950 - topic_output_recall_1: 0.4914 - val_loss: 1.4963 - val_sentiment_output_accuracy: 0.8238 - val_sentiment_output_loss: 0.2556 - val_sentiment_output_precision: 0.8247 - val_sentiment_output_recall: 0.8204 - val_topic_output_auc: 0.6220 - val_topic_output_loss: 1.2118 - val_topic_output_precision_1: 0.1045 - val_topic_output_recall_1: 0.5266 - learning_rate: 0.0010\nEpoch 6/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.2564 - sentiment_output_accuracy: 0.9446 - sentiment_output_loss: 0.0740 - sentiment_output_precision: 0.9483 - sentiment_output_recall: 0.9427 - topic_output_auc: 0.6285 - topic_output_loss: 1.1824 - topic_output_precision_1: 0.1030 - topic_output_recall_1: 0.5420 - val_loss: 1.4688 - val_sentiment_output_accuracy: 0.8290 - val_sentiment_output_loss: 0.2231 - val_sentiment_output_precision: 0.8307 - val_sentiment_output_recall: 0.8221 - val_topic_output_auc: 0.6006 - val_topic_output_loss: 1.2184 - val_topic_output_precision_1: 0.0955 - val_topic_output_recall_1: 0.5130 - learning_rate: 0.0010\nEpoch 7/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.2500 - sentiment_output_accuracy: 0.9591 - sentiment_output_loss: 0.0594 - sentiment_output_precision: 0.9626 - sentiment_output_recall: 0.9568 - topic_output_auc: 0.6028 - topic_output_loss: 1.1906 - topic_output_precision_1: 0.0946 - topic_output_recall_1: 0.4967 - val_loss: 1.5708 - val_sentiment_output_accuracy: 0.8238 - val_sentiment_output_loss: 0.3419 - val_sentiment_output_precision: 0.8336 - val_sentiment_output_recall: 0.8135 - val_topic_output_auc: 0.6460 - val_topic_output_loss: 1.1939 - val_topic_output_precision_1: 0.1124 - val_topic_output_recall_1: 0.5971 - learning_rate: 0.0010\nEpoch 8/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.2068 - sentiment_output_accuracy: 0.9679 - sentiment_output_loss: 0.0443 - sentiment_output_precision: 0.9737 - sentiment_output_recall: 0.9643 - topic_output_auc: 0.6438 - topic_output_loss: 1.1626 - topic_output_precision_1: 0.1081 - topic_output_recall_1: 0.5774 - val_loss: 1.5834 - val_sentiment_output_accuracy: 0.8221 - val_sentiment_output_loss: 0.3805 - val_sentiment_output_precision: 0.8275 - val_sentiment_output_recall: 0.8204 - val_topic_output_auc: 0.6473 - val_topic_output_loss: 1.1675 - val_topic_output_precision_1: 0.1118 - val_topic_output_recall_1: 0.5884 - learning_rate: 0.0010\nEpoch 9/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 1.1374 - sentiment_output_accuracy: 0.9728 - sentiment_output_loss: 0.0384 - sentiment_output_precision: 0.9749 - sentiment_output_recall: 0.9700 - topic_output_auc: 0.6555 - topic_output_loss: 1.0990 - topic_output_precision_1: 0.1090 - topic_output_recall_1: 0.5606 - val_loss: 1.5058 - val_sentiment_output_accuracy: 0.8446 - val_sentiment_output_loss: 0.3171 - val_sentiment_output_precision: 0.8536 - val_sentiment_output_recall: 0.8359 - val_topic_output_auc: 0.6623 - val_topic_output_loss: 1.1539 - val_topic_output_precision_1: 0.1154 - val_topic_output_recall_1: 0.6242 - learning_rate: 0.0010\nEpoch 10/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.1279 - sentiment_output_accuracy: 0.9571 - sentiment_output_loss: 0.0602 - sentiment_output_precision: 0.9615 - sentiment_output_recall: 0.9453 - topic_output_auc: 0.6813 - topic_output_loss: 1.0678 - topic_output_precision_1: 0.1169 - topic_output_recall_1: 0.5940 - val_loss: 1.4983 - val_sentiment_output_accuracy: 0.8394 - val_sentiment_output_loss: 0.3151 - val_sentiment_output_precision: 0.8399 - val_sentiment_output_recall: 0.8152 - val_topic_output_auc: 0.6713 - val_topic_output_loss: 1.1488 - val_topic_output_precision_1: 0.1193 - val_topic_output_recall_1: 0.6126 - learning_rate: 0.0010\nEpoch 11/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.1241 - sentiment_output_accuracy: 0.9625 - sentiment_output_loss: 0.0539 - sentiment_output_precision: 0.9666 - sentiment_output_recall: 0.9590 - topic_output_auc: 0.6878 - topic_output_loss: 1.0700 - topic_output_precision_1: 0.1203 - topic_output_recall_1: 0.5857 - val_loss: 1.5691 - val_sentiment_output_accuracy: 0.8463 - val_sentiment_output_loss: 0.3756 - val_sentiment_output_precision: 0.8456 - val_sentiment_output_recall: 0.8325 - val_topic_output_auc: 0.6962 - val_topic_output_loss: 1.1504 - val_topic_output_precision_1: 0.1259 - val_topic_output_recall_1: 0.6280 - learning_rate: 0.0010\nEpoch 12/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.0470 - sentiment_output_accuracy: 0.9735 - sentiment_output_loss: 0.0357 - sentiment_output_precision: 0.9738 - sentiment_output_recall: 0.9692 - topic_output_auc: 0.7196 - topic_output_loss: 1.0113 - topic_output_precision_1: 0.1295 - topic_output_recall_1: 0.6137 - val_loss: 1.7933 - val_sentiment_output_accuracy: 0.8152 - val_sentiment_output_loss: 0.6165 - val_sentiment_output_precision: 0.8222 - val_sentiment_output_recall: 0.8066 - val_topic_output_auc: 0.7313 - val_topic_output_loss: 1.1250 - val_topic_output_precision_1: 0.1440 - val_topic_output_recall_1: 0.6338 - learning_rate: 0.0010\nEpoch 13/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.0413 - sentiment_output_accuracy: 0.9585 - sentiment_output_loss: 0.0506 - sentiment_output_precision: 0.9624 - sentiment_output_recall: 0.9542 - topic_output_auc: 0.7422 - topic_output_loss: 0.9907 - topic_output_precision_1: 0.1417 - topic_output_recall_1: 0.6274 - val_loss: 1.5672 - val_sentiment_output_accuracy: 0.8169 - val_sentiment_output_loss: 0.4327 - val_sentiment_output_precision: 0.8214 - val_sentiment_output_recall: 0.8100 - val_topic_output_auc: 0.6938 - val_topic_output_loss: 1.1027 - val_topic_output_precision_1: 0.1278 - val_topic_output_recall_1: 0.5971 - learning_rate: 0.0010\nEpoch 14/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.0535 - sentiment_output_accuracy: 0.9657 - sentiment_output_loss: 0.0455 - sentiment_output_precision: 0.9700 - sentiment_output_recall: 0.9635 - topic_output_auc: 0.7391 - topic_output_loss: 1.0081 - topic_output_precision_1: 0.1460 - topic_output_recall_1: 0.6237 - val_loss: 1.6054 - val_sentiment_output_accuracy: 0.8135 - val_sentiment_output_loss: 0.4715 - val_sentiment_output_precision: 0.8188 - val_sentiment_output_recall: 0.8117 - val_topic_output_auc: 0.7393 - val_topic_output_loss: 1.0919 - val_topic_output_precision_1: 0.1483 - val_topic_output_recall_1: 0.6454 - learning_rate: 0.0010\nEpoch 15/15\n\u001b[1m73/73\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.9869 - sentiment_output_accuracy: 0.9756 - sentiment_output_loss: 0.0383 - sentiment_output_precision: 0.9756 - sentiment_output_recall: 0.9729 - topic_output_auc: 0.7629 - topic_output_loss: 0.9486 - topic_output_precision_1: 0.1562 - topic_output_recall_1: 0.6589 - val_loss: 1.5734 - val_sentiment_output_accuracy: 0.8428 - val_sentiment_output_loss: 0.4499 - val_sentiment_output_precision: 0.8467 - val_sentiment_output_recall: 0.8394 - val_topic_output_auc: 0.7584 - val_topic_output_loss: 1.0792 - val_topic_output_precision_1: 0.1539 - val_topic_output_recall_1: 0.6483 - learning_rate: 5.0000e-04\nƒê√£ l∆∞u m√¥ h√¨nh t·∫°i vietnamese_sentiment_model.keras\nƒê√£ l∆∞u √°nh x·∫° nh√£n t·∫°i vietnamese_sentiment_mappings.json\nƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validation...\n\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.6405 - sentiment_output_accuracy: 0.8315 - sentiment_output_loss: 0.4622 - sentiment_output_precision: 0.8291 - sentiment_output_recall: 0.8154 - topic_output_auc: 0.6979 - topic_output_loss: 1.1740 - topic_output_precision_1: 0.1266 - topic_output_recall_1: 0.6315\nloss: 1.5691\nsentiment_output_loss: 0.3756\ntopic_output_loss: 1.1504\nsentiment_output_accuracy: 0.8463\ntopic_output_auc: 0.8456\nsentiment_output_precision: 0.8325\nsentiment_output_recall: 0.6962\ntopic_output_precision: 0.1259\ntopic_output_recall: 0.6280\n\nƒê√°nh gi√° chi ti·∫øt sentiment:\n\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\nConfusion Matrix:\nTh·ª±c t·∫ø \\ D·ª± ƒëo√°n:\n    Ti√™u c·ª±c    Trung t√≠    T√≠ch c·ª±c    \nTi√™u c·ª±c         200           0           1\nTrung t√≠          28         125          35\nT√≠ch c·ª±c           2          23         165\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    Ti√™u c·ª±c     0.8696    0.9950    0.9281       201\n  Trung t√≠nh     0.8446    0.6649    0.7440       188\n    T√≠ch c·ª±c     0.8209    0.8684    0.8440       190\n\n    accuracy                         0.8463       579\n   macro avg     0.8450    0.8428    0.8387       579\nweighted avg     0.8455    0.8463    0.8407       579\n\n\nD·ª± ƒëo√°n ƒë∆°n:\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nVƒÉn b·∫£n: T√¥i r·∫•t th√≠ch s·∫£n ph·∫©m n√†y, ch·∫•t l∆∞·ª£ng tuy·ªát v·ªùi!\nT√¨nh c·∫£m: T√≠ch c·ª±c, ƒê·ªô tin c·∫≠y: 0.9007, M·ª©c ƒë·ªô tin c·∫≠y: cao\nCh·ªß ƒë·ªÅ: ['Du l·ªãch', 'VƒÉn ho√°', 'Covid-19', 'Khoa h·ªçc', 'Gi√°o d·ª•c']\nƒê·ªô tin c·∫≠y ch·ªß ƒë·ªÅ: ['0.6382', '0.5970', '0.5861', '0.5704', '0.5663']\n--------------------------------------------------\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nVƒÉn b·∫£n: D·ªãch v·ª• t·∫°i c·ª≠a h√†ng n√†y th·∫≠t t·ªá.\nT√¨nh c·∫£m: T√≠ch c·ª±c, ƒê·ªô tin c·∫≠y: 0.6807, M·ª©c ƒë·ªô tin c·∫≠y: trung b√¨nh\nCh·ªß ƒë·ªÅ: ['Gi√°o d·ª•c', 'Giao th√¥ng', 'Kinh t·∫ø', 'VƒÉn ho√°', 'Du l·ªãch']\nƒê·ªô tin c·∫≠y ch·ªß ƒë·ªÅ: ['0.5680', '0.5652', '0.5615', '0.5598', '0.5585']\n--------------------------------------------------\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nVƒÉn b·∫£n: S·∫£n ph·∫©m ƒë√∫ng nh∆∞ m√¥ t·∫£, giao h√†ng ƒë√∫ng h·∫πn.\nT√¨nh c·∫£m: T√≠ch c·ª±c, ƒê·ªô tin c·∫≠y: 0.6122, M·ª©c ƒë·ªô tin c·∫≠y: trung b√¨nh\nCh·ªß ƒë·ªÅ: ['Gi√°o d·ª•c', 'Giao th√¥ng', 'Du l·ªãch', 'VƒÉn ho√°', 'Khoa h·ªçc']\nƒê·ªô tin c·∫≠y ch·ªß ƒë·ªÅ: ['0.6256', '0.6165', '0.5825', '0.5768', '0.5743']\n--------------------------------------------------\n\nD·ª± ƒëo√°n h√†ng lo·∫°t:\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nVƒÉn b·∫£n: T√¥i r·∫•t th√≠ch s·∫£n ph·∫©m n√†y, ch·∫•t l∆∞·ª£ng tuy·ªát v·ªùi!\nT√¨nh c·∫£m: T√≠ch c·ª±c, ƒê·ªô tin c·∫≠y: 0.9007, M·ª©c ƒë·ªô: cao\nCh·ªß ƒë·ªÅ: ['Du l·ªãch', 'VƒÉn ho√°', 'Covid-19', 'Khoa h·ªçc', 'Gi√°o d·ª•c']\nƒê·ªô tin c·∫≠y ch·ªß ƒë·ªÅ: ['0.6382', '0.5970', '0.5861', '0.5704', '0.5663']\n--------------------------------------------------\nVƒÉn b·∫£n: D·ªãch v·ª• t·∫°i c·ª≠a h√†ng n√†y th·∫≠t t·ªá.\nT√¨nh c·∫£m: T√≠ch c·ª±c, ƒê·ªô tin c·∫≠y: 0.6807, M·ª©c ƒë·ªô: trung b√¨nh\nCh·ªß ƒë·ªÅ: ['Gi√°o d·ª•c', 'Giao th√¥ng', 'Kinh t·∫ø', 'VƒÉn ho√°', 'Du l·ªãch']\nƒê·ªô tin c·∫≠y ch·ªß ƒë·ªÅ: ['0.5680', '0.5652', '0.5615', '0.5598', '0.5585']\n--------------------------------------------------\nVƒÉn b·∫£n: S·∫£n ph·∫©m ƒë√∫ng nh∆∞ m√¥ t·∫£, giao h√†ng ƒë√∫ng h·∫πn.\nT√¨nh c·∫£m: T√≠ch c·ª±c, ƒê·ªô tin c·∫≠y: 0.6122, M·ª©c ƒë·ªô: trung b√¨nh\nCh·ªß ƒë·ªÅ: ['Gi√°o d·ª•c', 'Giao th√¥ng', 'Du l·ªãch', 'VƒÉn ho√°', 'Khoa h·ªçc']\nƒê·ªô tin c·∫≠y ch·ªß ƒë·ªÅ: ['0.6256', '0.6165', '0.5825', '0.5768', '0.5743']\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":5}]}