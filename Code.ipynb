{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91310,"databundleVersionId":10780789,"sourceType":"competition"},{"sourceId":11050229,"sourceType":"datasetVersion","datasetId":6884090}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T10:01:22.164708Z","iopub.execute_input":"2025-03-25T10:01:22.164979Z","iopub.status.idle":"2025-03-25T10:01:22.521012Z","shell.execute_reply.started":"2025-03-25T10:01:22.164947Z","shell.execute_reply":"2025-03-25T10:01:22.520035Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dpl-302-m-ai-1810-assignment-2/train data.json\n/kaggle/input/dpl-302-m-ai-1810-assignment-2/test.csv\n/kaggle/input/datata/train data.json\n/kaggle/input/datata/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nPhân loại tình cảm văn bản tiếng Việt sử dụng BiLSTM\nTối ưu cho tác vụ phân loại tình cảm (sentiment classification)\n\"\"\"\n\nimport os\nimport re\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom collections import Counter\nimport unicodedata  # Thêm thư viện xử lý Unicode\n\n# Cấu hình\nMAX_LENGTH = 100\nVOCAB_SIZE = 20000\nEMBEDDING_DIM = 128\nBATCH_SIZE = 32\nEPOCHS = 15\nLEARNING_RATE = 0.001  # Khôi phục learning rate ban đầu\n\nclass SentimentAttention(tf.keras.layers.Layer):\n    \"\"\"Lớp Attention tùy chỉnh cho phân loại cảm xúc\"\"\"\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(SentimentAttention, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        # Đảm bảo input_shape có thể là 2D hoặc 3D\n        feature_dim = input_shape[-1]\n        \n        # Tạo ma trận trọng số và bias\n        self.W = self.add_weight(\n            name=\"attention_weight\",\n            shape=(feature_dim, 1),\n            initializer=\"glorot_uniform\"\n        )\n        self.b = self.add_weight(\n            name=\"attention_bias\",\n            shape=(1,),\n            initializer=\"zeros\"\n        )\n        self.built = True\n        \n    def call(self, inputs, mask=None):\n        # Kiểm tra kích thước của inputs\n        input_shape = tf.shape(inputs)\n        \n        # Nếu chỉ có 2 chiều (batch_size, features) thì áp dụng attention trực tiếp\n        if len(inputs.shape) == 2:\n            # Khi input là từ dense layer, áp dụng dot product một lần\n            logits = tf.matmul(inputs, self.W) + self.b  # shape=(batch_size, 1)\n            attention_weights = tf.nn.sigmoid(logits)  # Sử dụng sigmoid thay vì softmax cho 1 phần tử\n            \n            # Tạo output = input (không cần weighted sum vì chỉ có 1 vector đặc trưng)\n            output = inputs\n            \n            return output, tf.squeeze(attention_weights, axis=-1)\n        \n        # Xử lý cho tensor 3D (batch_size, timesteps, features)\n        else:\n            # Áp dụng tanh(Wx + b) cho mỗi timestep\n            uit = tf.tanh(tf.matmul(\n                tf.reshape(inputs, [-1, input_shape[-1]]),  # Reshape để dot product\n                self.W\n            ) + self.b)\n            \n            # Reshape về (batch_size, timesteps, 1)\n            uit = tf.reshape(uit, [-1, input_shape[1], 1])\n            \n            # Áp dụng mask nếu có\n            if mask is not None:\n                mask = tf.cast(mask, tf.bool)\n                mask = tf.expand_dims(mask, axis=-1)  # (batch_size, timesteps, 1)\n                paddings = tf.ones_like(uit) * (-1e9)\n                uit = tf.where(mask, uit, paddings)\n            \n            # Tính softmax theo chiều timesteps\n            attention_weights = tf.nn.softmax(uit, axis=1)  # (batch_size, timesteps, 1)\n            \n            # Áp dụng attention weights cho inputs\n            weighted_input = inputs * attention_weights\n            \n            # Tổng hợp theo chiều timesteps\n            output = tf.reduce_sum(weighted_input, axis=1)  # (batch_size, features)\n            \n            return output, tf.squeeze(attention_weights, axis=-1)\n    \n    def compute_output_shape(self, input_shape):\n        # Output là (batch_size, features) và (batch_size, sequence_length)\n        if len(input_shape) == 3:\n            return [(input_shape[0], input_shape[2]), (input_shape[0], input_shape[1])]\n        else:  # Trường hợp 2D\n            return [(input_shape[0], input_shape[1]), (input_shape[0], 1)]\n    \n    def get_config(self):\n        config = super(SentimentAttention, self).get_config()\n        return config\n\nclass VietnameseTextClassifier:\n    def __init__(self):\n        print(\"Khởi tạo Vietnamese Text Classifier với mô hình BiLSTM...\")\n        self.tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n        self.topic_threshold = 0.2  # Giảm ngưỡng chủ đề từ 0.5 xuống 0.2\n        \n    def load_train_data(self, json_path):\n        \"\"\"Tải dữ liệu huấn luyện từ file JSON (hỗ trợ định dạng Label Studio)\"\"\"\n        print(f\"Đang tải dữ liệu huấn luyện từ {json_path}...\")\n        \n        try:\n            # Kiểm tra file tồn tại\n            if not os.path.exists(json_path):\n                print(f\"Lỗi: Không tìm thấy file {json_path}\")\n                return pd.DataFrame({'text': [], 'sentiment': [], 'topics': []})\n            \n            # Tải dữ liệu JSON\n            with open(json_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            # In thông tin debug\n            print(f\"Cấu trúc dữ liệu: {type(data)}\")\n            if isinstance(data, list):\n                print(f\"Số lượng mục: {len(data)}\")\n            \n            # Xử lý dữ liệu\n            rows = []\n            \n            # Xử lý định dạng Label Studio\n            if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict) and 'annotations' in data[0]:\n                print(\"Phát hiện định dạng Label Studio\")\n                \n                for item in data:\n                    # Trích xuất văn bản\n                    text = \"\"\n                    if 'data' in item and isinstance(item['data'], dict):\n                        for field in ['text', 'content', 'document']:\n                            if field in item['data'] and isinstance(item['data'][field], str):\n                                text = item['data'][field].strip()\n                                break\n                    \n                    if not text:\n                        continue\n                    \n                    # Trích xuất tình cảm và chủ đề\n                    sentiment = \"Trung tính\"  # Mặc định\n                    topics = []  # Danh sách chủ đề\n                    \n                    if 'annotations' in item and isinstance(item['annotations'], list):\n                        for annotation in item['annotations']:\n                            if 'value' in annotation and isinstance(annotation['value'], dict):\n                                # Tìm tình cảm trong choices\n                                if 'choices' in annotation['value'] and isinstance(annotation['value']['choices'], list):\n                                    for choice in annotation['value']['choices']:\n                                        if choice in [\"Tích cực\", \"Tiêu cực\", \"Trung tính\"]:\n                                            sentiment = choice\n                                        else:\n                                            # Nếu không phải tình cảm, có thể là chủ đề\n                                            topics.append(choice)\n                                \n                                # Tìm chủ đề trong labels\n                                if 'labels' in annotation['value'] and isinstance(annotation['value']['labels'], list):\n                                    topics.extend(annotation['value']['labels'])\n                    \n                    # Thêm vào danh sách\n                    rows.append({\n                        'text': self.clean_text(text),\n                        'sentiment': sentiment,\n                        'topics': topics\n                    })\n            else:\n                # Xử lý định dạng thông thường\n                for item in data if isinstance(data, list) else [data]:\n                    if isinstance(item, dict) and 'text' in item:\n                        topics = []\n                        if 'topics' in item and isinstance(item['topics'], list):\n                            topics = item['topics']\n                        elif 'labels' in item and isinstance(item['labels'], list):\n                            topics = item['labels']\n                        \n                        rows.append({\n                            'text': self.clean_text(item['text']),\n                            'sentiment': item.get('sentiment', \"Trung tính\"),\n                            'topics': topics\n                        })\n            \n            # Tạo DataFrame\n            df = pd.DataFrame(rows)\n            print(f\"Đã tải {len(df)} mẫu dữ liệu huấn luyện\")\n            \n            # Kiểm tra chủ đề\n            if 'topics' in df.columns:\n                all_topics = set()\n                for topics_list in df['topics']:\n                    if isinstance(topics_list, list):\n                        all_topics.update(topics_list)\n                print(f\"Tìm thấy {len(all_topics)} chủ đề khác nhau\")\n            \n            return df\n            \n        except Exception as e:\n            print(f\"Lỗi khi tải dữ liệu huấn luyện: {str(e)}\")\n            return pd.DataFrame({'text': [], 'sentiment': [], 'topics': []})\n    \n    def clean_text(self, text):\n        \"\"\"Làm sạch văn bản tiếng Việt với các cải tiến cho phân loại cảm xúc\"\"\"\n        if not isinstance(text, str):\n            return \"\"\n        \n        # Chuẩn hóa Unicode (NFC)\n        text = unicodedata.normalize('NFC', text)\n        \n        # Thay thế emojis bằng mô tả\n        text = self._replace_emojis(text)\n        \n        # Xử lý kí tự lặp (ví dụ: \"quáaaaaa\" -> \"quá\")\n        text = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', text)\n        \n        # Chuẩn hóa dấu câu quan trọng cho cảm xúc\n        text = re.sub(r'!{2,}', ' ! ! ', text)  # Chuẩn hóa nhiều dấu chấm than\n        text = re.sub(r'\\?{2,}', ' ? ? ', text)  # Chuẩn hóa nhiều dấu hỏi\n        \n        # Thay thế nhiều khoảng trắng bằng một khoảng trắng\n        text = re.sub(r'\\s+', ' ', text)\n        # Loại bỏ URL và thẻ HTML\n        text = re.sub(r'https?://\\S+|www\\.\\S+|<.*?>', '', text)\n        # Loại bỏ khoảng trắng thừa\n        return text.strip()\n    \n    def _replace_emojis(self, text):\n        \"\"\"Thay thế emojis phổ biến bằng từ mô tả cảm xúc\"\"\"\n        emoji_map = {\n            '😊': ' vui_mừng ',\n            '😄': ' cười_lớn ',\n            '😃': ' vui_vẻ ',\n            '😀': ' cười ',\n            '😍': ' yêu_thích ',\n            '🥰': ' rất_yêu_thích ',\n            '😘': ' hôn_gió ',\n            '❤️': ' trái_tim ',\n            '♥️': ' yêu ',\n            '👍': ' thích ',\n            '👎': ' không_thích ',\n            '😢': ' buồn ',\n            '😭': ' khóc ',\n            '😡': ' tức_giận ',\n            '🤬': ' rất_tức_giận ',\n            '🤔': ' suy_nghĩ ',\n            '😱': ' sợ_hãi ',\n            '🙄': ' ngán_ngẩm ',\n            '🤣': ' cười_lăn ',\n            '😂': ' cười_ra_nước_mắt '\n        }\n        \n        for emoji, replacement in emoji_map.items():\n            text = text.replace(emoji, replacement)\n        \n        return text\n    \n    def prepare_data(self, train_df):\n        \"\"\"Chuẩn bị dữ liệu cho huấn luyện và kiểm tra (sử dụng split từ dữ liệu JSON)\"\"\"\n        if len(train_df) == 0:\n            print(\"Cảnh báo: DataFrame huấn luyện rỗng, không thể chuẩn bị dữ liệu\")\n            return None, None\n        \n        # Tạo ánh xạ nhãn tình cảm\n        unique_sentiments = sorted(list(set(train_df['sentiment'])))\n        sentiment_to_idx = {sentiment: i for i, sentiment in enumerate(unique_sentiments)}\n        \n        # Tạo ánh xạ nhãn chủ đề\n        all_topics = set()\n        if 'topics' in train_df.columns:\n            for topics_list in train_df['topics']:\n                if isinstance(topics_list, list):\n                    all_topics.update(topics_list)\n        \n        unique_topics = sorted(list(all_topics))\n        topic_to_idx = {topic: i for i, topic in enumerate(unique_topics)}\n        \n        # Cân bằng dữ liệu sentiment trước khi chia\n        # Đếm số lượng mẫu cho mỗi nhãn cảm xúc\n        sentiment_counts = train_df['sentiment'].value_counts()\n        max_samples = sentiment_counts.max()\n        \n        # Tăng cường dữ liệu cho các nhóm thiểu số\n        augmented_rows = []\n        for sentiment, count in sentiment_counts.items():\n            if count < max_samples:\n                # Lấy tất cả các mẫu của sentiment này\n                sentiment_samples = train_df[train_df['sentiment'] == sentiment]\n                # Số lượng mẫu cần tạo thêm\n                num_to_generate = max_samples - count\n                \n                # Tạo thêm mẫu bằng cách nhân bản và thêm biến thể nhẹ\n                for i in range(num_to_generate):\n                    # Chọn ngẫu nhiên một mẫu để nhân bản\n                    sample = sentiment_samples.sample(1).iloc[0]\n                    \n                    # Tạo biến thể nhẹ cho text\n                    original_text = sample['text']\n                    augmented_text = self._augment_text(original_text)\n                    \n                    # Tạo mẫu mới\n                    augmented_row = sample.copy()\n                    augmented_row['text'] = augmented_text\n                    \n                    augmented_rows.append(augmented_row)\n        \n        # Thêm các mẫu tăng cường vào DataFrame\n        if augmented_rows:\n            augmented_df = pd.DataFrame(augmented_rows)\n            train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n            print(f\"Đã tạo thêm {len(augmented_rows)} mẫu tăng cường để cân bằng dữ liệu cảm xúc\")\n        \n        # Chia dữ liệu huấn luyện và validation\n        train_split_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n        print(f\"Chia dữ liệu: {len(train_split_df)} mẫu huấn luyện, {len(val_df)} mẫu validation\")\n        \n        # Huấn luyện tokenizer trên tập huấn luyện\n        self.tokenizer.fit_on_texts(train_split_df['text'])\n        \n        # Chuyển đổi văn bản thành chuỗi số\n        train_sequences = self.tokenizer.texts_to_sequences(train_split_df['text'])\n        val_sequences = self.tokenizer.texts_to_sequences(val_df['text'])\n        \n        # Đệm chuỗi để có cùng độ dài\n        train_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post')\n        val_padded = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post')\n        \n        # Chuẩn bị nhãn tình cảm\n        train_sentiment_labels = self._prepare_sentiment_labels(train_split_df['sentiment'].tolist(), sentiment_to_idx)\n        val_sentiment_labels = self._prepare_sentiment_labels(val_df['sentiment'].tolist(), sentiment_to_idx)\n        \n        # Chuẩn bị nhãn chủ đề (multi-label)\n        train_topic_labels = self._prepare_topic_labels(train_split_df['topics'].tolist(), topic_to_idx)\n        val_topic_labels = self._prepare_topic_labels(val_df['topics'].tolist(), topic_to_idx)\n        \n        # Tạo dictionary dữ liệu\n        data = {\n            'train': {\n                'sequences': train_padded,\n                'sentiment_labels': train_sentiment_labels,\n                'topic_labels': train_topic_labels\n            },\n            'val': {\n                'sequences': val_padded,\n                'sentiment_labels': val_sentiment_labels,\n                'topic_labels': val_topic_labels\n            }\n        }\n        \n        # Tạo ánh xạ nhãn và từ vựng\n        mappings = {\n            'unique_sentiments': unique_sentiments,\n            'sentiment_to_idx': sentiment_to_idx,\n            'idx_to_sentiment': {i: sentiment for sentiment, i in sentiment_to_idx.items()},\n            'num_sentiments': len(sentiment_to_idx),\n            'unique_topics': unique_topics,\n            'topic_to_idx': topic_to_idx,\n            'idx_to_topic': {i: topic for topic, i in topic_to_idx.items()},\n            'num_topics': len(topic_to_idx),\n            'word_index': self.tokenizer.word_index,\n            'vocab_size': min(VOCAB_SIZE, len(self.tokenizer.word_index) + 1)\n        }\n        \n        return data, mappings\n    \n    def _augment_text(self, text):\n        \"\"\"Tạo biến thể nhẹ của văn bản để tăng cường dữ liệu\"\"\"\n        if not isinstance(text, str) or len(text) < 10:\n            return text\n            \n        augmented = text\n        \n        # Danh sách các phép biến đổi\n        transforms = [\n            self._add_emphasis,\n            self._swap_words,\n            self._synonym_replacement,\n            self._remove_random_words,\n        ]\n        \n        # Chọn ngẫu nhiên 1-2 phép biến đổi\n        num_transforms = np.random.randint(1, 3)\n        selected_transforms = np.random.choice(transforms, num_transforms, replace=False)\n        \n        # Áp dụng các phép biến đổi\n        for transform in selected_transforms:\n            augmented = transform(augmented)\n            \n        return augmented\n    \n    def _add_emphasis(self, text):\n        \"\"\"Thêm dấu câu nhấn mạnh cảm xúc\"\"\"\n        # Một số từ quan trọng và emoji tương ứng\n        emphasis = [\n            (\"!\", \"!!\"),\n            (\"?\", \"??\"),\n            (\"rất\", \"rất rất\"),\n            (\"tuyệt\", \"tuyệt vời\"),\n            (\"tốt\", \"rất tốt\"),\n            (\"không tốt\", \"không tốt chút nào\"),\n            (\"tệ\", \"quá tệ\"),\n        ]\n        \n        result = text\n        # Chọn ngẫu nhiên 1 cách nhấn mạnh để áp dụng\n        choice = np.random.choice(len(emphasis))\n        original, replacement = emphasis[choice]\n        \n        # Thay thế với xác suất 0.7\n        if original in result and np.random.random() < 0.7:\n            result = result.replace(original, replacement, 1)\n            \n        return result\n    \n    def _swap_words(self, text):\n        \"\"\"Hoán đổi vị trí của hai từ cạnh nhau\"\"\"\n        words = text.split()\n        if len(words) < 4:\n            return text\n            \n        # Chọn ngẫu nhiên vị trí để hoán đổi, tránh từ đầu và cuối\n        idx = np.random.randint(1, len(words) - 2)\n        \n        # Hoán đổi từ\n        words[idx], words[idx + 1] = words[idx + 1], words[idx]\n        \n        return \" \".join(words)\n    \n    def _synonym_replacement(self, text):\n        \"\"\"Thay thế từ bằng từ đồng nghĩa\"\"\"\n        # Một số từ đồng nghĩa cơ bản cho tình cảm\n        synonyms = {\n            \"tốt\": [\"hay\", \"tuyệt\", \"tích cực\", \"khá\"],\n            \"tuyệt\": [\"tuyệt vời\", \"xuất sắc\", \"hoàn hảo\"],\n            \"tệ\": [\"kém\", \"tồi\", \"không tốt\", \"dở\"],\n            \"thích\": [\"yêu thích\", \"ưa\", \"hài lòng\"],\n            \"buồn\": [\"không vui\", \"thất vọng\", \"chán nản\"],\n            \"vui\": [\"vui vẻ\", \"hạnh phúc\", \"phấn khởi\"]\n        }\n        \n        words = text.split()\n        for i, word in enumerate(words):\n            if word in synonyms and np.random.random() < 0.3:\n                # Thay thế bằng từ đồng nghĩa ngẫu nhiên\n                replacement = np.random.choice(synonyms[word])\n                words[i] = replacement\n                break  # Chỉ thay thế một từ\n                \n        return \" \".join(words)\n    \n    def _remove_random_words(self, text):\n        \"\"\"Loại bỏ một từ ngẫu nhiên\"\"\"\n        words = text.split()\n        if len(words) < 5:\n            return text\n            \n        # Loại bỏ một từ ngẫu nhiên không quan trọng (tránh từ đầu tiên)\n        idx = np.random.randint(1, len(words))\n        words.pop(idx)\n        \n        return \" \".join(words)\n    \n    def _prepare_sentiment_labels(self, sentiments, sentiment_to_idx):\n        \"\"\"Chuẩn bị nhãn tình cảm (single-label)\"\"\"\n        labels = []\n        for sentiment in sentiments:\n            if sentiment in sentiment_to_idx:\n                idx = sentiment_to_idx[sentiment]\n            else:\n                # Nếu không tìm thấy, sử dụng Trung tính (hoặc chỉ số thích hợp)\n                idx = sentiment_to_idx.get(\"Trung tính\", 0)\n            labels.append(idx)\n        \n        return tf.keras.utils.to_categorical(labels, num_classes=len(sentiment_to_idx))\n    \n    def _prepare_topic_labels(self, topics_list, topic_to_idx):\n        \"\"\"Chuẩn bị nhãn chủ đề (multi-label)\"\"\"\n        num_topics = len(topic_to_idx)\n        labels = np.zeros((len(topics_list), num_topics))\n        \n        for i, topics in enumerate(topics_list):\n            if isinstance(topics, list):\n                for topic in topics:\n                    if topic in topic_to_idx:\n                        labels[i, topic_to_idx[topic]] = 1\n        \n        return labels\n    \n    def build_model(self, mappings):\n        \"\"\"Xây dựng mô hình BiLSTM cho phân loại tình cảm và chủ đề\"\"\"\n        print(\"Xây dựng mô hình BiLSTM kép cho phân loại tình cảm và chủ đề...\")\n        \n        # Lấy kích thước từ vựng và số lớp\n        vocab_size = mappings['vocab_size']\n        num_sentiments = mappings['num_sentiments']\n        num_topics = mappings['num_topics']\n        \n        # Đầu vào \n        input_layer = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_sequences')\n        \n        # Embedding layer\n        embedding = tf.keras.layers.Embedding(\n            vocab_size, EMBEDDING_DIM, input_length=MAX_LENGTH\n        )(input_layer)\n        \n        # Shared BiLSTM layers\n        lstm1 = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(128, return_sequences=True)\n        )(embedding)\n        lstm1_dropout = tf.keras.layers.Dropout(0.3)(lstm1)\n        \n        lstm2 = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(64, return_sequences=True)\n        )(lstm1_dropout)\n        lstm2_dropout = tf.keras.layers.Dropout(0.3)(lstm2)\n        \n        lstm3 = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(32)\n        )(lstm2_dropout)\n        lstm3_dropout = tf.keras.layers.Dropout(0.3)(lstm3)\n        \n        # Shared Dense layers\n        shared_dense1 = tf.keras.layers.Dense(256, activation='relu')(lstm3_dropout)\n        shared_dense1_dropout = tf.keras.layers.Dropout(0.3)(shared_dense1)\n        \n        shared_dense2 = tf.keras.layers.Dense(128, activation='relu')(shared_dense1_dropout)\n        shared_dense2_dropout = tf.keras.layers.Dropout(0.3)(shared_dense2)\n        \n        # Sentiment specific layers\n        sentiment_attention = SentimentAttention()(shared_dense2_dropout)\n        sentiment_dense = tf.keras.layers.Dense(64, activation='relu')(sentiment_attention[0])\n        sentiment_dropout = tf.keras.layers.Dropout(0.1)(sentiment_dense)\n        sentiment_output = tf.keras.layers.Dense(\n            num_sentiments, activation='softmax', name='sentiment_output'\n        )(sentiment_dropout)\n        \n        # Topic specific layers - thêm nhiều lớp dense hơn cho tác vụ multi-label phức tạp\n        topic_dense1 = tf.keras.layers.Dense(128, activation='relu')(shared_dense2_dropout)\n        topic_dropout1 = tf.keras.layers.Dropout(0.2)(topic_dense1)\n        \n        topic_dense2 = tf.keras.layers.Dense(64, activation='relu')(topic_dropout1)\n        topic_dropout2 = tf.keras.layers.Dropout(0.2)(topic_dense2)\n        \n        topic_output = tf.keras.layers.Dense(\n            num_topics, activation='sigmoid', name='topic_output'\n        )(topic_dropout2)\n        \n        # Tạo mô hình\n        model = tf.keras.Model(\n            inputs=input_layer,\n            outputs=[sentiment_output, topic_output]\n        )\n        \n        return model\n    \n    def train(self, data, mappings):\n        \"\"\"Huấn luyện mô hình\"\"\"\n        print(\"Huấn luyện mô hình...\")\n        \n        # Tạo mô hình\n        model = self.build_model(mappings)\n        \n        # Tính toán class weights cho sentiment\n        unique_sentiment_counts = Counter(np.argmax(data['train']['sentiment_labels'], axis=1))\n        total_sentiment_samples = sum(unique_sentiment_counts.values())\n        sentiment_class_weights = {\n            class_idx: total_sentiment_samples / (count * len(unique_sentiment_counts))\n            for class_idx, count in unique_sentiment_counts.items()\n        }\n        \n        print(\"Tỷ lệ cảm xúc:\")\n        for class_idx, weight in sentiment_class_weights.items():\n            sentiment_name = mappings['idx_to_sentiment'][class_idx]\n            count = unique_sentiment_counts[class_idx]\n            print(f\"  - {sentiment_name}: {count}/{total_sentiment_samples} mẫu ({count/total_sentiment_samples*100:.2f}%) - weight: {weight:.2f}\")\n        \n        # Topic weights (sử dụng positive_weights cho các nhãn hiếm)\n        topic_sums = np.sum(data['train']['topic_labels'], axis=0)\n        total_samples = data['train']['topic_labels'].shape[0]\n        pos_weights = np.zeros(mappings['num_topics'])\n        \n        for i in range(mappings['num_topics']):\n            # Cân bằng tỷ lệ âm/dương cho mỗi chủ đề\n            if topic_sums[i] > 0:\n                neg_samples = total_samples - topic_sums[i]\n                pos_weights[i] = neg_samples / (topic_sums[i] * 1.0)\n            else:\n                pos_weights[i] = 10.0  # giá trị mặc định cao nếu không có mẫu dương\n        \n        # Positive topic samples để tăng cường huấn luyện\n        print(\"Tỷ lệ chủ đề:\")\n        for i, topic in enumerate(mappings['unique_topics']):\n            pos_count = topic_sums[i] \n            print(f\"  - {topic}: {pos_count}/{total_samples} mẫu ({pos_count/total_samples*100:.2f}%) - weight: {pos_weights[i]:.2f}\")\n        \n        # Tăng cường dữ liệu cân bằng cho sentiment\n        X_train = data['train']['sequences']\n        y_train_sentiment = data['train']['sentiment_labels']\n        y_train_topic = data['train']['topic_labels']\n        \n        # Xác định lớp thiểu số và đa số\n        minority_class = min(unique_sentiment_counts, key=unique_sentiment_counts.get)\n        majority_classes = [i for i in unique_sentiment_counts.keys() if i != minority_class]\n        \n        # Thêm focal loss cho sentiment để tập trung vào các mẫu khó phân loại\n        def focal_loss(gamma=2.0, alpha=0.25):\n            def focal_loss_fn(y_true, y_pred):\n                # Clip values để tránh log(0)\n                y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n                \n                # Focal loss formula\n                cross_entropy = -y_true * tf.math.log(y_pred)\n                weight = tf.pow(1 - y_pred, gamma) * y_true\n                \n                # Apply weights từ class weights\n                weights = np.ones(mappings['num_sentiments'])\n                for i in range(mappings['num_sentiments']):\n                    if i in sentiment_class_weights:\n                        weights[i] = sentiment_class_weights[i]\n                \n                weight_tensor = tf.constant(weights, dtype=tf.float32)\n                weighted_focal_loss = cross_entropy * weight * tf.expand_dims(weight_tensor, 0)\n                \n                return tf.reduce_sum(weighted_focal_loss, axis=-1)\n            return focal_loss_fn\n        \n        # Weighted binary crossentropy cho topic với positive weights\n        def weighted_binary_crossentropy(y_true, y_pred):\n            # Áp dụng positive_weights cho từng chủ đề\n            weights = tf.constant(pos_weights, dtype=tf.float32)\n            y_true = tf.cast(y_true, tf.float32)\n            \n            # Tránh log(0)\n            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n            \n            # Công thức weighted binary crossentropy\n            pos_term = -y_true * tf.math.log(y_pred) * weights  # positive samples có trọng số cao hơn\n            neg_term = -(1 - y_true) * tf.math.log(1 - y_pred)\n            \n            return tf.reduce_mean(pos_term + neg_term, axis=-1)\n        \n        # Biên dịch lại mô hình với các trọng số\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n            loss={\n                'sentiment_output': focal_loss(gamma=2.0),  # Dùng focal loss thay vì weighted categorical\n                'topic_output': weighted_binary_crossentropy\n            },\n            loss_weights={\n                'sentiment_output': 1.5,  # Tăng trọng số cho sentiment\n                'topic_output': 1.0\n            },\n            metrics={\n                'sentiment_output': ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n                'topic_output': [tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n            }\n        )\n        \n        # Callbacks\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_sentiment_output_accuracy',\n                patience=10,  # Tăng patience để cho mô hình thối gian hội tụ\n                restore_best_weights=True,\n                mode='max'\n            ),\n            tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_sentiment_output_accuracy',\n                factor=0.5,\n                patience=3,\n                min_lr=1e-6,\n                mode='max'\n            ),\n            # Thêm checkpoint để lưu mô hình tốt nhất theo sentiment accuracy\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath='best_sentiment_model.keras',\n                save_best_only=True,\n                monitor='val_sentiment_output_accuracy',\n                mode='max'\n            )\n        ]\n        \n        try:\n            # Huấn luyện\n            history = model.fit(\n                data['train']['sequences'],\n                {\n                    'sentiment_output': data['train']['sentiment_labels'],\n                    'topic_output': data['train']['topic_labels']\n                },\n                validation_data=(\n                    data['val']['sequences'],\n                    {\n                        'sentiment_output': data['val']['sentiment_labels'],\n                        'topic_output': data['val']['topic_labels']\n                    }\n                ),\n                epochs=EPOCHS,\n                batch_size=BATCH_SIZE,\n                callbacks=callbacks\n                # Đã loại bỏ class_weight vì không hỗ trợ cho mô hình đa đầu ra\n            )\n            \n            # Lưu lịch sử huấn luyện để phân tích sau\n            self.history = history.history\n            \n            # Lưu mô hình\n            self.save_model(model, mappings)\n            \n            return model\n        except Exception as e:\n            print(f\"Lỗi khi huấn luyện mô hình: {str(e)}\")\n            return None\n    \n    def predict_sentiment(self, model, text, mappings):\n        \"\"\"Dự đoán tình cảm cho văn bản với các cải tiến\"\"\"\n        if model is None or mappings is None:\n            print(\"Không có mô hình hoặc ánh xạ nhãn\")\n            return \"Không xác định\", 0.0\n        \n        # Tiền xử lý văn bản\n        cleaned_text = self.clean_text(text)\n        sequence = self.tokenizer.texts_to_sequences([cleaned_text])\n        padded = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n        \n        # Tạo nhiều biến thể của một câu để tăng cường hiệu suất dự đoán\n        # Ví dụ: thêm dấu câu biểu thị cảm xúc, thay đổi từ ngữ nhẹ, v.v.\n        variations = [\n            padded,  # Câu gốc\n        ]\n        \n        # Dự đoán\n        try:\n            # Dự đoán tất cả các biến thể\n            all_predictions = []\n            \n            for var in variations:\n                predictions = model.predict(var)\n                all_predictions.append(predictions[0][0])  # Lấy kết quả sentiment\n            \n            # Lấy trung bình của tất cả các dự đoán\n            avg_predictions = np.mean(all_predictions, axis=0)\n            \n            # Xử lý kết quả tình cảm\n            sentiment_idx = np.argmax(avg_predictions)\n            sentiment = mappings['idx_to_sentiment'][sentiment_idx]\n            confidence = float(avg_predictions[sentiment_idx])\n            \n            # Thêm logic để xác định mức độ chắc chắn\n            confidence_level = \"cao\" if confidence > 0.8 else \"trung bình\" if confidence > 0.6 else \"thấp\"\n            \n            return sentiment, confidence, confidence_level\n        except Exception as e:\n            print(f\"Lỗi khi dự đoán: {str(e)}\")\n            return \"Không xác định\", 0.0, \"không xác định\"\n    \n    def predict_topics(self, model, text, mappings, threshold=None, max_topics=5):\n        \"\"\"Dự đoán chủ đề cho văn bản\n        \n        Args:\n            model: Mô hình đã huấn luyện\n            text: Văn bản cần dự đoán\n            mappings: Bản đồ ánh xạ nhãn\n            threshold: Ngưỡng xác suất để chọn chủ đề (mặc định: 0.2)\n            max_topics: Số lượng chủ đề tối đa trả về (mặc định: 5)\n            \n        Returns:\n            Tuple (selected_topics, confidences): Danh sách chủ đề và độ tin cậy tương ứng\n        \"\"\"\n        if model is None or mappings is None:\n            print(\"Không có mô hình hoặc ánh xạ nhãn\")\n            return [], []\n        \n        if threshold is None:\n            threshold = self.topic_threshold\n            \n        # Tiền xử lý văn bản\n        cleaned_text = self.clean_text(text)\n        sequence = self.tokenizer.texts_to_sequences([cleaned_text])\n        padded = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n        \n        # Dự đoán\n        try:\n            predictions = model.predict(padded)\n            \n            # Xử lý kết quả chủ đề (lấy đầu ra thứ hai là topics)\n            topic_preds = predictions[1][0]  # Lấy dự đoán chủ đề\n            \n            # Tạo danh sách các chủ đề với xác suất\n            topics_with_probs = [(mappings['idx_to_topic'][i], float(prob)) \n                             for i, prob in enumerate(topic_preds) \n                             if prob >= threshold]\n            \n            # Sắp xếp theo độ tin cậy giảm dần và chỉ lấy tối đa max_topics chủ đề\n            topics_with_probs.sort(key=lambda x: x[1], reverse=True)\n            topics_with_probs = topics_with_probs[:max_topics]\n            \n            # Tách thành hai danh sách riêng biệt\n            if topics_with_probs:\n                selected_topics, confidences = zip(*topics_with_probs)\n                return list(selected_topics), list(confidences)\n            else:\n                return [], []\n                \n        except Exception as e:\n            print(f\"Lỗi khi dự đoán chủ đề: {str(e)}\")\n            return [], []\n            \n    def predict_batch(self, model, texts, mappings, topic_threshold=None, max_topics=5):\n        \"\"\"Dự đoán tình cảm và chủ đề cho một loạt văn bản\n        \n        Args:\n            model: Mô hình đã huấn luyện\n            texts: Danh sách các văn bản cần dự đoán\n            mappings: Bản đồ ánh xạ nhãn\n            topic_threshold: Ngưỡng xác suất để chọn chủ đề (mặc định: 0.2)\n            max_topics: Số lượng chủ đề tối đa trả về (mặc định: 5)\n            \n        Returns:\n            Danh sách kết quả dự đoán, mỗi kết quả bao gồm text, sentiment, sentiment_confidence, topics, topic_confidences\n        \"\"\"\n        if model is None or mappings is None:\n            print(\"Không có mô hình hoặc ánh xạ nhãn\")\n            return []\n        \n        if topic_threshold is None:\n            topic_threshold = self.topic_threshold\n            \n        # Làm sạch và tiền xử lý văn bản\n        cleaned_texts = [self.clean_text(text) for text in texts]\n        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)\n        padded = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post')\n        \n        # Dự đoán\n        try:\n            predictions = model.predict(padded)\n            sentiment_preds = predictions[0]  # [batch_size, num_sentiments]\n            topic_preds = predictions[1]  # [batch_size, num_topics]\n            \n            # Xử lý kết quả\n            results = []\n            for i, (sent_pred, topic_pred) in enumerate(zip(sentiment_preds, topic_preds)):\n                # Sentiment\n                sentiment_idx = np.argmax(sent_pred)\n                sentiment = mappings['idx_to_sentiment'][sentiment_idx]\n                sentiment_confidence = float(sent_pred[sentiment_idx])\n                confidence_level = \"cao\" if sentiment_confidence > 0.8 else \"trung bình\" if sentiment_confidence > 0.6 else \"thấp\"\n                \n                # Topics - tạo danh sách với xác suất\n                topics_with_probs = [(mappings['idx_to_topic'][j], float(prob)) \n                                 for j, prob in enumerate(topic_pred) \n                                 if prob >= topic_threshold]\n                \n                # Sắp xếp theo độ tin cậy giảm dần và chỉ lấy tối đa max_topics chủ đề\n                topics_with_probs.sort(key=lambda x: x[1], reverse=True)\n                topics_with_probs = topics_with_probs[:max_topics]\n                \n                # Tách thành hai danh sách riêng biệt\n                if topics_with_probs:\n                    selected_topics, topic_confidences = zip(*topics_with_probs)\n                    selected_topics = list(selected_topics)\n                    topic_confidences = list(topic_confidences)\n                else:\n                    selected_topics, topic_confidences = [], []\n                \n                results.append({\n                    'text': texts[i],\n                    'sentiment': sentiment,\n                    'sentiment_confidence': sentiment_confidence,\n                    'confidence_level': confidence_level,\n                    'topics': selected_topics,\n                    'topic_confidences': topic_confidences\n                })\n            \n            return results\n        except Exception as e:\n            print(f\"Lỗi khi dự đoán hàng loạt: {str(e)}\")\n            return []\n    \n    def evaluate(self, model, data, mappings):\n        \"\"\"Đánh giá mô hình trên tập validation với tập trung vào độ chính xác cảm xúc\"\"\"\n        if model is None or data is None or mappings is None:\n            print(\"Không có mô hình, dữ liệu hoặc ánh xạ nhãn để đánh giá\")\n            return None\n        \n        try:\n            # Đánh giá\n            print(\"Đánh giá mô hình trên tập validation...\")\n            results = model.evaluate(\n                data['val']['sequences'],\n                {\n                    'sentiment_output': data['val']['sentiment_labels'],\n                    'topic_output': data['val']['topic_labels']\n                }\n            )\n            \n            # In kết quả\n            metrics = ['loss', 'sentiment_output_loss', 'topic_output_loss', \n                       'sentiment_output_accuracy', 'topic_output_auc', \n                       'sentiment_output_precision', 'sentiment_output_recall', \n                       'topic_output_precision', 'topic_output_recall']\n            for i, metric in enumerate(metrics):\n                if i < len(results):\n                    print(f\"{metric}: {results[i]:.4f}\")\n            \n            # Phân tích chi tiết hơn cho sentiment\n            print(\"\\nĐánh giá chi tiết sentiment:\")\n            \n            # Lấy dự đoán\n            predictions = model.predict(data['val']['sequences'])\n            sentiment_preds = predictions[0]\n            \n            # Chuyển đổi về nhãn số\n            true_labels = np.argmax(data['val']['sentiment_labels'], axis=1)\n            pred_labels = np.argmax(sentiment_preds, axis=1)\n            \n            # Tính confusion matrix\n            from sklearn.metrics import confusion_matrix, classification_report\n            \n            cm = confusion_matrix(true_labels, pred_labels)\n            print(\"Confusion Matrix:\")\n            sentiment_names = [mappings['idx_to_sentiment'][i] for i in range(mappings['num_sentiments'])]\n            \n            # In confusion matrix dễ đọc\n            print(\"Thực tế \\\\ Dự đoán:\")\n            header = \"    \" + \"\".join([f\"{name[:8]:12s}\" for name in sentiment_names])\n            print(header)\n            \n            for i, row in enumerate(cm):\n                print(f\"{sentiment_names[i][:8]:8s} {' '.join([f'{x:11d}' for x in row])}\")\n            \n            # In classification report\n            print(\"\\nClassification Report:\")\n            report = classification_report(\n                true_labels, pred_labels,\n                target_names=sentiment_names,\n                digits=4\n            )\n            print(report)\n            \n            return results\n        except Exception as e:\n            print(f\"Lỗi khi đánh giá mô hình: {str(e)}\")\n            return None\n    \n    def save_model(self, model, mappings):\n        \"\"\"Lưu mô hình và ánh xạ nhãn\"\"\"\n        model_save_path = \"vietnamese_sentiment_model.keras\"  \n        model.save(model_save_path)\n        \n        # Lưu mappings để sử dụng khi tải mô hình\n        mappings_path = \"vietnamese_sentiment_mappings.json\"\n        with open(mappings_path, 'w', encoding='utf-8') as f:\n            # Chuyển đổi các key từ int sang str để có thể lưu thành JSON\n            serializable_mappings = {\n                'unique_sentiments': mappings['unique_sentiments'],\n                'sentiment_to_idx': mappings['sentiment_to_idx'],\n                'idx_to_sentiment': {str(i): s for i, s in mappings['idx_to_sentiment'].items()},\n                'num_sentiments': mappings['num_sentiments'],\n                'unique_topics': mappings['unique_topics'],\n                'topic_to_idx': mappings['topic_to_idx'],\n                'idx_to_topic': {str(i): t for i, t in mappings['idx_to_topic'].items()},\n                'num_topics': mappings['num_topics'],\n                'vocab_size': mappings['vocab_size']\n            }\n            json.dump(serializable_mappings, f, ensure_ascii=False, indent=2)\n        \n        print(f\"Đã lưu mô hình tại {model_save_path}\")\n        print(f\"Đã lưu ánh xạ nhãn tại {mappings_path}\")\n        \n    def load_model(self, model_path=None, mappings_path=None):\n        \"\"\"Tải mô hình đã huấn luyện\"\"\"\n        # Đường dẫn mặc định\n        if model_path is None:\n            model_path = \"vietnamese_sentiment_model.keras\"\n        if mappings_path is None:\n            mappings_path = \"vietnamese_sentiment_mappings.json\"\n            \n        try:\n            # Kiểm tra tập tin tồn tại\n            if not os.path.exists(model_path):\n                print(f\"Không tìm thấy file mô hình tại {model_path}\")\n                return None, None\n                \n            if not os.path.exists(mappings_path):\n                print(f\"Không tìm thấy file ánh xạ nhãn tại {mappings_path}\")\n                return None, None\n            \n            # Tải mappings\n            with open(mappings_path, 'r', encoding='utf-8') as f:\n                loaded_mappings = json.load(f)\n                \n            # Chuyển đổi key từ str về int\n            if 'idx_to_sentiment' in loaded_mappings:\n                loaded_mappings['idx_to_sentiment'] = {int(i): s for i, s in loaded_mappings['idx_to_sentiment'].items()}\n            if 'idx_to_topic' in loaded_mappings:\n                loaded_mappings['idx_to_topic'] = {int(i): t for i, t in loaded_mappings['idx_to_topic'].items()}\n            \n            # Tạo các hàm loss tùy chỉnh trống để tải mô hình\n            # (chúng sẽ được cập nhật lại khi huấn luyện)\n            def custom_sentiment_loss(y_true, y_pred):\n                return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n                \n            def custom_topic_loss(y_true, y_pred):\n                return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n                \n            # Tải mô hình\n            model = tf.keras.models.load_model(\n                model_path,\n                custom_objects={\n                    'weighted_categorical_crossentropy': custom_sentiment_loss,\n                    'weighted_binary_crossentropy': custom_topic_loss\n                }\n            )\n            \n            print(f\"Đã tải mô hình từ {model_path}\")\n            return model, loaded_mappings\n            \n        except Exception as e:\n            print(f\"Lỗi khi tải mô hình: {str(e)}\")\n            return None, None\n\n# Ví dụ sử dụng\nif __name__ == \"__main__\":\n    try:\n        # Khởi tạo classifier\n        classifier = VietnameseTextClassifier()\n        \n        # Tải dữ liệu huấn luyện JSON\n        json_path = \"/kaggle/input/datata/train data.json\"  # Đường dẫn đến file JSON\n        \n        # Kiểm tra file JSON tồn tại\n        if not os.path.exists(json_path):\n            print(f\"Không tìm thấy file dữ liệu tại {json_path}\")\n            # Thử tải mô hình đã huấn luyện\n            model, mappings = classifier.load_model()\n        else:\n            train_df = classifier.load_train_data(json_path)\n            \n            # Chuẩn bị dữ liệu (tự động tạo tập validation từ dữ liệu huấn luyện)\n            data, mappings = classifier.prepare_data(train_df)\n            \n            # Huấn luyện mô hình\n            model = classifier.train(data, mappings)\n            \n            # Đánh giá mô hình\n            if model is not None:\n                classifier.evaluate(model, data, mappings)\n        \n        # Ví dụ dự đoán\n        example_texts = [\n            \"Tôi rất thích sản phẩm này, chất lượng tuyệt vời!\",\n            \"Dịch vụ tại cửa hàng này thật tệ.\",\n            \"Sản phẩm đúng như mô tả, giao hàng đúng hẹn.\"\n        ]\n        \n        # Dự đoán dùng mô hình hiện tại\n        if model is not None and mappings is not None:\n            print(\"\\nDự đoán đơn:\")\n            for text in example_texts:\n                sentiment, confidence, confidence_level = classifier.predict_sentiment(model, text, mappings)\n                topics, topic_conf = classifier.predict_topics(model, text, mappings)\n                \n                print(f\"Văn bản: {text}\")\n                print(f\"Tình cảm: {sentiment}, Độ tin cậy: {confidence:.4f}, Mức độ tin cậy: {confidence_level}\")\n                print(f\"Chủ đề: {topics}\")\n                print(f\"Độ tin cậy chủ đề: {[f'{conf:.4f}' for conf in topic_conf]}\")\n                print(\"-\" * 50)\n            \n            print(\"\\nDự đoán hàng loạt:\")\n            batch_results = classifier.predict_batch(model, example_texts, mappings)\n            for result in batch_results:\n                print(f\"Văn bản: {result['text']}\")\n                print(f\"Tình cảm: {result['sentiment']}, Độ tin cậy: {result['sentiment_confidence']:.4f}, Mức độ: {result['confidence_level']}\")\n                print(f\"Chủ đề: {result['topics']}\")\n                print(f\"Độ tin cậy chủ đề: {[f'{conf:.4f}' for conf in result['topic_confidences']]}\")\n                print(\"-\" * 50)\n        else:\n            print(\"Không thể dự đoán do không có mô hình hoặc ánh xạ nhãn\")\n        \n    except Exception as e:\n        print(f\"Lỗi: {str(e)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T10:14:59.158371Z","iopub.execute_input":"2025-03-25T10:14:59.158720Z","iopub.status.idle":"2025-03-25T10:15:53.452477Z","shell.execute_reply.started":"2025-03-25T10:14:59.158694Z","shell.execute_reply":"2025-03-25T10:15:53.451665Z"}},"outputs":[{"name":"stdout","text":"Khởi tạo Vietnamese Text Classifier với mô hình BiLSTM...\nĐang tải dữ liệu huấn luyện từ /kaggle/input/datata/train data.json...\nCấu trúc dữ liệu: <class 'list'>\nSố lượng mục: 1696\nPhát hiện định dạng Label Studio\nĐã tải 1695 mẫu dữ liệu huấn luyện\nTìm thấy 23 chủ đề khác nhau\nĐã tạo thêm 1197 mẫu tăng cường để cân bằng dữ liệu cảm xúc\nChia dữ liệu: 2313 mẫu huấn luyện, 579 mẫu validation\nHuấn luyện mô hình...\nXây dựng mô hình BiLSTM kép cho phân loại tình cảm và chủ đề...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Tỷ lệ cảm xúc:\n  - Trung tính: 776/2313 mẫu (33.55%) - weight: 0.99\n  - Tiêu cực: 763/2313 mẫu (32.99%) - weight: 1.01\n  - Tích cực: 774/2313 mẫu (33.46%) - weight: 1.00\nTỷ lệ chủ đề:\n  - Bất động sản: 86.0/2313 mẫu (3.72%) - weight: 25.90\n  - Chính trị: 84.0/2313 mẫu (3.63%) - weight: 26.54\n  - Chứng khoán: 43.0/2313 mẫu (1.86%) - weight: 52.79\n  - Covid-19: 292.0/2313 mẫu (12.62%) - weight: 6.92\n  - Công nghệ: 154.0/2313 mẫu (6.66%) - weight: 14.02\n  - Du lịch: 56.0/2313 mẫu (2.42%) - weight: 40.30\n  - Game: 24.0/2313 mẫu (1.04%) - weight: 95.38\n  - Giao thông: 44.0/2313 mẫu (1.90%) - weight: 51.57\n  - Giáo dục: 61.0/2313 mẫu (2.64%) - weight: 36.92\n  - Giải trí: 292.0/2313 mẫu (12.62%) - weight: 6.92\n  - Hóng biến: 257.0/2313 mẫu (11.11%) - weight: 8.00\n  - Khoa học: 33.0/2313 mẫu (1.43%) - weight: 69.09\n  - Không xác định: 24.0/2313 mẫu (1.04%) - weight: 95.38\n  - Kinh tế: 405.0/2313 mẫu (17.51%) - weight: 4.71\n  - Môi trường: 102.0/2313 mẫu (4.41%) - weight: 21.68\n  - Phim ảnh: 54.0/2313 mẫu (2.33%) - weight: 41.83\n  - Pháp luật: 172.0/2313 mẫu (7.44%) - weight: 12.45\n  - Sức khoẻ: 209.0/2313 mẫu (9.04%) - weight: 10.07\n  - Thế giới: 287.0/2313 mẫu (12.41%) - weight: 7.06\n  - Thể thao: 188.0/2313 mẫu (8.13%) - weight: 11.30\n  - Văn hoá: 116.0/2313 mẫu (5.02%) - weight: 18.94\n  - Xã hội: 394.0/2313 mẫu (17.03%) - weight: 4.87\n  - Đời sống: 684.0/2313 mẫu (29.57%) - weight: 2.38\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['attention_weight', 'attention_bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - loss: 2.0238 - sentiment_output_accuracy: 0.3341 - sentiment_output_loss: 0.7348 - sentiment_output_precision: 0.0000e+00 - sentiment_output_recall: 0.0000e+00 - topic_output_auc: 0.5098 - topic_output_loss: 1.2891 - topic_output_precision_1: 0.0795 - topic_output_recall_1: 0.3941 - val_loss: 2.0015 - val_sentiment_output_accuracy: 0.5492 - val_sentiment_output_loss: 0.6998 - val_sentiment_output_precision: 0.0000e+00 - val_sentiment_output_recall: 0.0000e+00 - val_topic_output_auc: 0.4919 - val_topic_output_loss: 1.2826 - val_topic_output_precision_1: 0.0813 - val_topic_output_recall_1: 0.3691 - learning_rate: 0.0010\nEpoch 2/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.8208 - sentiment_output_accuracy: 0.5937 - sentiment_output_loss: 0.5440 - sentiment_output_precision: 0.6437 - sentiment_output_recall: 0.2484 - topic_output_auc: 0.5425 - topic_output_loss: 1.2767 - topic_output_precision_1: 0.0837 - topic_output_recall_1: 0.4392 - val_loss: 1.6103 - val_sentiment_output_accuracy: 0.7168 - val_sentiment_output_loss: 0.3331 - val_sentiment_output_precision: 0.7995 - val_sentiment_output_recall: 0.5164 - val_topic_output_auc: 0.5910 - val_topic_output_loss: 1.2465 - val_topic_output_precision_1: 0.0927 - val_topic_output_recall_1: 0.4928 - learning_rate: 0.0010\nEpoch 3/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.4898 - sentiment_output_accuracy: 0.7680 - sentiment_output_loss: 0.2458 - sentiment_output_precision: 0.7967 - sentiment_output_recall: 0.6839 - topic_output_auc: 0.5889 - topic_output_loss: 1.2440 - topic_output_precision_1: 0.0932 - topic_output_recall_1: 0.5023 - val_loss: 1.4847 - val_sentiment_output_accuracy: 0.7858 - val_sentiment_output_loss: 0.2276 - val_sentiment_output_precision: 0.8086 - val_sentiment_output_recall: 0.7444 - val_topic_output_auc: 0.5847 - val_topic_output_loss: 1.2298 - val_topic_output_precision_1: 0.0887 - val_topic_output_recall_1: 0.5169 - learning_rate: 0.0010\nEpoch 4/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 1.3727 - sentiment_output_accuracy: 0.8788 - sentiment_output_loss: 0.1510 - sentiment_output_precision: 0.8861 - sentiment_output_recall: 0.8630 - topic_output_auc: 0.5897 - topic_output_loss: 1.2216 - topic_output_precision_1: 0.0921 - topic_output_recall_1: 0.5236 - val_loss: 1.4831 - val_sentiment_output_accuracy: 0.8117 - val_sentiment_output_loss: 0.2372 - val_sentiment_output_precision: 0.8209 - val_sentiment_output_recall: 0.7997 - val_topic_output_auc: 0.6198 - val_topic_output_loss: 1.2178 - val_topic_output_precision_1: 0.1029 - val_topic_output_recall_1: 0.5227 - learning_rate: 0.0010\nEpoch 5/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.2913 - sentiment_output_accuracy: 0.9360 - sentiment_output_loss: 0.0796 - sentiment_output_precision: 0.9418 - sentiment_output_recall: 0.9297 - topic_output_auc: 0.6049 - topic_output_loss: 1.2117 - topic_output_precision_1: 0.0950 - topic_output_recall_1: 0.4914 - val_loss: 1.4963 - val_sentiment_output_accuracy: 0.8238 - val_sentiment_output_loss: 0.2556 - val_sentiment_output_precision: 0.8247 - val_sentiment_output_recall: 0.8204 - val_topic_output_auc: 0.6220 - val_topic_output_loss: 1.2118 - val_topic_output_precision_1: 0.1045 - val_topic_output_recall_1: 0.5266 - learning_rate: 0.0010\nEpoch 6/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.2564 - sentiment_output_accuracy: 0.9446 - sentiment_output_loss: 0.0740 - sentiment_output_precision: 0.9483 - sentiment_output_recall: 0.9427 - topic_output_auc: 0.6285 - topic_output_loss: 1.1824 - topic_output_precision_1: 0.1030 - topic_output_recall_1: 0.5420 - val_loss: 1.4688 - val_sentiment_output_accuracy: 0.8290 - val_sentiment_output_loss: 0.2231 - val_sentiment_output_precision: 0.8307 - val_sentiment_output_recall: 0.8221 - val_topic_output_auc: 0.6006 - val_topic_output_loss: 1.2184 - val_topic_output_precision_1: 0.0955 - val_topic_output_recall_1: 0.5130 - learning_rate: 0.0010\nEpoch 7/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.2500 - sentiment_output_accuracy: 0.9591 - sentiment_output_loss: 0.0594 - sentiment_output_precision: 0.9626 - sentiment_output_recall: 0.9568 - topic_output_auc: 0.6028 - topic_output_loss: 1.1906 - topic_output_precision_1: 0.0946 - topic_output_recall_1: 0.4967 - val_loss: 1.5708 - val_sentiment_output_accuracy: 0.8238 - val_sentiment_output_loss: 0.3419 - val_sentiment_output_precision: 0.8336 - val_sentiment_output_recall: 0.8135 - val_topic_output_auc: 0.6460 - val_topic_output_loss: 1.1939 - val_topic_output_precision_1: 0.1124 - val_topic_output_recall_1: 0.5971 - learning_rate: 0.0010\nEpoch 8/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.2068 - sentiment_output_accuracy: 0.9679 - sentiment_output_loss: 0.0443 - sentiment_output_precision: 0.9737 - sentiment_output_recall: 0.9643 - topic_output_auc: 0.6438 - topic_output_loss: 1.1626 - topic_output_precision_1: 0.1081 - topic_output_recall_1: 0.5774 - val_loss: 1.5834 - val_sentiment_output_accuracy: 0.8221 - val_sentiment_output_loss: 0.3805 - val_sentiment_output_precision: 0.8275 - val_sentiment_output_recall: 0.8204 - val_topic_output_auc: 0.6473 - val_topic_output_loss: 1.1675 - val_topic_output_precision_1: 0.1118 - val_topic_output_recall_1: 0.5884 - learning_rate: 0.0010\nEpoch 9/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 1.1374 - sentiment_output_accuracy: 0.9728 - sentiment_output_loss: 0.0384 - sentiment_output_precision: 0.9749 - sentiment_output_recall: 0.9700 - topic_output_auc: 0.6555 - topic_output_loss: 1.0990 - topic_output_precision_1: 0.1090 - topic_output_recall_1: 0.5606 - val_loss: 1.5058 - val_sentiment_output_accuracy: 0.8446 - val_sentiment_output_loss: 0.3171 - val_sentiment_output_precision: 0.8536 - val_sentiment_output_recall: 0.8359 - val_topic_output_auc: 0.6623 - val_topic_output_loss: 1.1539 - val_topic_output_precision_1: 0.1154 - val_topic_output_recall_1: 0.6242 - learning_rate: 0.0010\nEpoch 10/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.1279 - sentiment_output_accuracy: 0.9571 - sentiment_output_loss: 0.0602 - sentiment_output_precision: 0.9615 - sentiment_output_recall: 0.9453 - topic_output_auc: 0.6813 - topic_output_loss: 1.0678 - topic_output_precision_1: 0.1169 - topic_output_recall_1: 0.5940 - val_loss: 1.4983 - val_sentiment_output_accuracy: 0.8394 - val_sentiment_output_loss: 0.3151 - val_sentiment_output_precision: 0.8399 - val_sentiment_output_recall: 0.8152 - val_topic_output_auc: 0.6713 - val_topic_output_loss: 1.1488 - val_topic_output_precision_1: 0.1193 - val_topic_output_recall_1: 0.6126 - learning_rate: 0.0010\nEpoch 11/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 1.1241 - sentiment_output_accuracy: 0.9625 - sentiment_output_loss: 0.0539 - sentiment_output_precision: 0.9666 - sentiment_output_recall: 0.9590 - topic_output_auc: 0.6878 - topic_output_loss: 1.0700 - topic_output_precision_1: 0.1203 - topic_output_recall_1: 0.5857 - val_loss: 1.5691 - val_sentiment_output_accuracy: 0.8463 - val_sentiment_output_loss: 0.3756 - val_sentiment_output_precision: 0.8456 - val_sentiment_output_recall: 0.8325 - val_topic_output_auc: 0.6962 - val_topic_output_loss: 1.1504 - val_topic_output_precision_1: 0.1259 - val_topic_output_recall_1: 0.6280 - learning_rate: 0.0010\nEpoch 12/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.0470 - sentiment_output_accuracy: 0.9735 - sentiment_output_loss: 0.0357 - sentiment_output_precision: 0.9738 - sentiment_output_recall: 0.9692 - topic_output_auc: 0.7196 - topic_output_loss: 1.0113 - topic_output_precision_1: 0.1295 - topic_output_recall_1: 0.6137 - val_loss: 1.7933 - val_sentiment_output_accuracy: 0.8152 - val_sentiment_output_loss: 0.6165 - val_sentiment_output_precision: 0.8222 - val_sentiment_output_recall: 0.8066 - val_topic_output_auc: 0.7313 - val_topic_output_loss: 1.1250 - val_topic_output_precision_1: 0.1440 - val_topic_output_recall_1: 0.6338 - learning_rate: 0.0010\nEpoch 13/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.0413 - sentiment_output_accuracy: 0.9585 - sentiment_output_loss: 0.0506 - sentiment_output_precision: 0.9624 - sentiment_output_recall: 0.9542 - topic_output_auc: 0.7422 - topic_output_loss: 0.9907 - topic_output_precision_1: 0.1417 - topic_output_recall_1: 0.6274 - val_loss: 1.5672 - val_sentiment_output_accuracy: 0.8169 - val_sentiment_output_loss: 0.4327 - val_sentiment_output_precision: 0.8214 - val_sentiment_output_recall: 0.8100 - val_topic_output_auc: 0.6938 - val_topic_output_loss: 1.1027 - val_topic_output_precision_1: 0.1278 - val_topic_output_recall_1: 0.5971 - learning_rate: 0.0010\nEpoch 14/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 1.0535 - sentiment_output_accuracy: 0.9657 - sentiment_output_loss: 0.0455 - sentiment_output_precision: 0.9700 - sentiment_output_recall: 0.9635 - topic_output_auc: 0.7391 - topic_output_loss: 1.0081 - topic_output_precision_1: 0.1460 - topic_output_recall_1: 0.6237 - val_loss: 1.6054 - val_sentiment_output_accuracy: 0.8135 - val_sentiment_output_loss: 0.4715 - val_sentiment_output_precision: 0.8188 - val_sentiment_output_recall: 0.8117 - val_topic_output_auc: 0.7393 - val_topic_output_loss: 1.0919 - val_topic_output_precision_1: 0.1483 - val_topic_output_recall_1: 0.6454 - learning_rate: 0.0010\nEpoch 15/15\n\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.9869 - sentiment_output_accuracy: 0.9756 - sentiment_output_loss: 0.0383 - sentiment_output_precision: 0.9756 - sentiment_output_recall: 0.9729 - topic_output_auc: 0.7629 - topic_output_loss: 0.9486 - topic_output_precision_1: 0.1562 - topic_output_recall_1: 0.6589 - val_loss: 1.5734 - val_sentiment_output_accuracy: 0.8428 - val_sentiment_output_loss: 0.4499 - val_sentiment_output_precision: 0.8467 - val_sentiment_output_recall: 0.8394 - val_topic_output_auc: 0.7584 - val_topic_output_loss: 1.0792 - val_topic_output_precision_1: 0.1539 - val_topic_output_recall_1: 0.6483 - learning_rate: 5.0000e-04\nĐã lưu mô hình tại vietnamese_sentiment_model.keras\nĐã lưu ánh xạ nhãn tại vietnamese_sentiment_mappings.json\nĐánh giá mô hình trên tập validation...\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.6405 - sentiment_output_accuracy: 0.8315 - sentiment_output_loss: 0.4622 - sentiment_output_precision: 0.8291 - sentiment_output_recall: 0.8154 - topic_output_auc: 0.6979 - topic_output_loss: 1.1740 - topic_output_precision_1: 0.1266 - topic_output_recall_1: 0.6315\nloss: 1.5691\nsentiment_output_loss: 0.3756\ntopic_output_loss: 1.1504\nsentiment_output_accuracy: 0.8463\ntopic_output_auc: 0.8456\nsentiment_output_precision: 0.8325\nsentiment_output_recall: 0.6962\ntopic_output_precision: 0.1259\ntopic_output_recall: 0.6280\n\nĐánh giá chi tiết sentiment:\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\nConfusion Matrix:\nThực tế \\ Dự đoán:\n    Tiêu cực    Trung tí    Tích cực    \nTiêu cực         200           0           1\nTrung tí          28         125          35\nTích cực           2          23         165\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    Tiêu cực     0.8696    0.9950    0.9281       201\n  Trung tính     0.8446    0.6649    0.7440       188\n    Tích cực     0.8209    0.8684    0.8440       190\n\n    accuracy                         0.8463       579\n   macro avg     0.8450    0.8428    0.8387       579\nweighted avg     0.8455    0.8463    0.8407       579\n\n\nDự đoán đơn:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nVăn bản: Tôi rất thích sản phẩm này, chất lượng tuyệt vời!\nTình cảm: Tích cực, Độ tin cậy: 0.9007, Mức độ tin cậy: cao\nChủ đề: ['Du lịch', 'Văn hoá', 'Covid-19', 'Khoa học', 'Giáo dục']\nĐộ tin cậy chủ đề: ['0.6382', '0.5970', '0.5861', '0.5704', '0.5663']\n--------------------------------------------------\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nVăn bản: Dịch vụ tại cửa hàng này thật tệ.\nTình cảm: Tích cực, Độ tin cậy: 0.6807, Mức độ tin cậy: trung bình\nChủ đề: ['Giáo dục', 'Giao thông', 'Kinh tế', 'Văn hoá', 'Du lịch']\nĐộ tin cậy chủ đề: ['0.5680', '0.5652', '0.5615', '0.5598', '0.5585']\n--------------------------------------------------\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nVăn bản: Sản phẩm đúng như mô tả, giao hàng đúng hẹn.\nTình cảm: Tích cực, Độ tin cậy: 0.6122, Mức độ tin cậy: trung bình\nChủ đề: ['Giáo dục', 'Giao thông', 'Du lịch', 'Văn hoá', 'Khoa học']\nĐộ tin cậy chủ đề: ['0.6256', '0.6165', '0.5825', '0.5768', '0.5743']\n--------------------------------------------------\n\nDự đoán hàng loạt:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nVăn bản: Tôi rất thích sản phẩm này, chất lượng tuyệt vời!\nTình cảm: Tích cực, Độ tin cậy: 0.9007, Mức độ: cao\nChủ đề: ['Du lịch', 'Văn hoá', 'Covid-19', 'Khoa học', 'Giáo dục']\nĐộ tin cậy chủ đề: ['0.6382', '0.5970', '0.5861', '0.5704', '0.5663']\n--------------------------------------------------\nVăn bản: Dịch vụ tại cửa hàng này thật tệ.\nTình cảm: Tích cực, Độ tin cậy: 0.6807, Mức độ: trung bình\nChủ đề: ['Giáo dục', 'Giao thông', 'Kinh tế', 'Văn hoá', 'Du lịch']\nĐộ tin cậy chủ đề: ['0.5680', '0.5652', '0.5615', '0.5598', '0.5585']\n--------------------------------------------------\nVăn bản: Sản phẩm đúng như mô tả, giao hàng đúng hẹn.\nTình cảm: Tích cực, Độ tin cậy: 0.6122, Mức độ: trung bình\nChủ đề: ['Giáo dục', 'Giao thông', 'Du lịch', 'Văn hoá', 'Khoa học']\nĐộ tin cậy chủ đề: ['0.6256', '0.6165', '0.5825', '0.5768', '0.5743']\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":5}]}